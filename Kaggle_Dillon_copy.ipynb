{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Flatten, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras import regularizers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very slightly cleaned up version of the ipython notebook that I used to train models for the Kaggle competition. I've attempted to add a comment at the top of each cell to say roughly what that cell did. The code is pretty haphazard because a lot of what we did was exploratory. But I feel like it generally reflects the evolution of our models. Given more time, we would do more systematic optimization of our final model (the sentiment sorted, bagged CNN). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "train_with_labels = pd.read_csv('training_data.txt',sep=' ')\n",
    "y = train_with_labels['Label']\n",
    "train = train_with_labels.drop('Label',1)\n",
    "test = pd.read_csv('test_data.txt',sep=' ')\n",
    "joint_set = pd.concat([train,test])\n",
    "\n",
    "#Further data preprocessing\n",
    "words = [str(k) for k in joint_set.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Count of word appearence. \n",
    "#Consider pruning words that are too ubiquitous (and seem to be neutral) or not used enough\n",
    "\n",
    "word_count = []\n",
    "\n",
    "for word in words:\n",
    "    word_count.append((np.sum(train[word]),word))\n",
    "    #word_count.append((np.sum(test[word]),word))\n",
    "    #word_count.append((np.sum(joint_set[word]),word))\n",
    "    \n",
    "#sorted(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'Histogram of review lengths (counting only top 1000 words)')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEWCAYAAABFSLFOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9150e0650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check for reviews with no words <- this prob doesn't matter\n",
    "\n",
    "values = Counter(train.sum(axis=1)).values()\n",
    "plt.hist(values,bins=30)\n",
    "plt.xlabel('Length (words)')\n",
    "plt.ylabel('Number of top 1000 words in review')\n",
    "plt.title('Histogram of review lengths (counting only top 1000 words)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Individual word sentiment - this gets the sentiment for each individual word\n",
    "\n",
    "def get_sentiment(word, weighted = False):\n",
    "    '''\n",
    "    Input: word\n",
    "    Output: average sentiment (1 for 4/5 star, 0 for 1/2 star of that word)\n",
    "    If weighted == True: it counts each instance of word as a separate review for the purpose of average sentiment\n",
    "    '''\n",
    "    \n",
    "    labels = np.array(train_with_labels.loc[train_with_labels[word]>0]['Label'])\n",
    "    \n",
    "    \n",
    "    return round(np.mean(labels),3)\n",
    "\n",
    "\n",
    "\n",
    "word_sentiment = []\n",
    "\n",
    "for word in words:\n",
    "    sentiment = get_sentiment(word)\n",
    "    word_sentiment.append((np.sum(train[word]),sentiment,word))\n",
    "    \n",
    "#sorted(word_sentiment)\n",
    "word_sentiment.sort(key=lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,u'Word frequency')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCMzMas6JwMys5pwIzMxqzonAzKzm/j8vyNxQ0G02bwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8e33490d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMysNA4qZmZWGgcVMzMrjYOKmZmV5v8DfcESHCZZIvUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc90eb62f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make histogram of word sentiments. it might be possible to cut off some of the neutral words\n",
    "sentiments = np.array([tup[1] for tup in word_sentiment])\n",
    "freq = [tup[0] for tup in word_sentiment]\n",
    "ordered_words = [tup[2] for tup in word_sentiment]\n",
    "\n",
    "plt.hist(sentiments,bins=40)\n",
    "plt.title('Average sentiment of all words')\n",
    "plt.xlabel('Average sentiment')\n",
    "plt.ylabel('Number of words')\n",
    "plt.figure()\n",
    "plt.scatter(sentiments,freq)\n",
    "plt.title('Word frequency vs sentiment')\n",
    "plt.xlabel('Average sentiment')\n",
    "plt.ylabel('Word frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 407)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'great', u'love', u'dont', u'even', u'buy', u'well', u'best', u'cd',\n",
       "       u'album', u'song',\n",
       "       ...\n",
       "       u'aspect', u'jump', u'summer', u'brought', u'shock', u'secret',\n",
       "       u'complaint', u'challeng', u'halloween', u'soni'],\n",
       "      dtype='object', length=407)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop anything between 0.4 and 0.6 in sentiment\n",
    "neutral_words = np.array(ordered_words)[np.where((sentiments<0.6)&(sentiments>0.4))[0]]\n",
    "sparse_train = train.drop(neutral_words,axis=1)\n",
    "sparse_test = test.drop(neutral_words,axis=1)\n",
    "print np.shape(sparse_train)\n",
    "sparse_words = sparse_train.keys()\n",
    "sparse_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to figure out what words are actually in a given review\n",
    "def get_review(vector,sparse=True):\n",
    "    if sparse:\n",
    "        return sparse_words[np.where(vector != 0)]\n",
    "    elif sparse==False:\n",
    "        return np.array(words)[np.where(vector != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The stuff that is commented out with the docstring triple quotes below is the naive classifier\n",
    "\n",
    "train_indices = np.random.randint(0,len(train),15000)\n",
    "X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(sparse_train,y,test_size=0.25,random_state = 47)]\n",
    "'''\n",
    "#Do a stupid test\n",
    "stupid_preds = []\n",
    "for n in range(len(X_train)):\n",
    "    review = get_review(X_train[n],sparse=True)\n",
    "    score = np.mean([get_sentiment(i) for i in review])\n",
    "    if score > 0.5:\n",
    "        stupid_preds.append(1)\n",
    "    else:\n",
    "        stupid_preds.append(0)\n",
    "        \n",
    "print (np.array(stupid_preds) == y_train).sum()/float(len(stupid_preds)) \n",
    "#This is after pruning the 0.4 to 0.6 words\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train test split using scikit learn\n",
    "\n",
    "X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(train,y,test_size=0.25,random_state = 47)]\n",
    "#X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(sparse_train,y,test_size=0.25,random_state = 47)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First pass at a dense neural network by Dillon\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(1000,),kernel_regularizer=regularizers.l1_l2(0.001)))\n",
    "#model.add(Dense(407, input_shape=(407,),kernel_regularizer=regularizers.l1_l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#model.add(Dense(100))\n",
    "#model.add(Dropout(0.4))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10,kernel_regularizer=regularizers.l1(0.001)))\n",
    "#model.add(Dropout(0.4))\n",
    "#model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "\n",
    "print 'Number of parameters = ',model.count_params()\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "print train_acc, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run this cell for more epochs of training if desired\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "print train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Summarize the model\n",
    "\n",
    "for layer in model.layers:\n",
    "    print layer.name, layer.input_shape, layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#At this point, Devon had a better performing dense neural net so I switched to that\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(BatchNormalization(axis=1, input_shape=(1000,), momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "model.add(Dense(1000, input_shape=(1000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "\n",
    "model.add(Dense(500,kernel_regularizer=regularizers.l1_l2(0.0005)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(500,kernel_regularizer=regularizers.l1_l2(0.0005)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(500,kernel_regularizer=regularizers.l1_l2(0.0005)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "print 'Number of parameters = ',model.count_params()\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "print train_acc, test_acc\n",
    "if test_acc > record:\n",
    "    record = test_acc\n",
    "    print 'New record: ', test_acc\n",
    "    print 'Write down what you did'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Again for more training epochs\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "print train_acc, test_acc\n",
    "if test_acc > record:\n",
    "    record = test_acc\n",
    "    print 'New record: ', test_acc\n",
    "    print 'Write down what you did'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test of regularization parameter\n",
    "\n",
    "test_accs = []\n",
    "regs = np.linspace(0.00005,0.001,10)\n",
    "for reg in regs:\n",
    "    #Devon's code\n",
    "    print 'testing', reg\n",
    "\n",
    "    ## Create your own model here given the constraints in the problem\n",
    "    model = Sequential()\n",
    "    #model.add(BatchNormalization(axis=1, input_shape=(1000,), momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "    model.add(Dense(1000, input_shape=(1000,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.35))\n",
    "    \n",
    "    model.add(Dense(200,kernel_regularizer=regularizers.l1_l2(reg)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.35))\n",
    " \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print 'Number of parameters = ',model.count_params()\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=6, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "    train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "    test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "    \n",
    "    test_accs.append((np.max(history.history['val_acc']),np.argmax(history.history['val_acc'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I didn't include this plot in the final report because I'd have to run the code again\n",
    "#Also it wasn't the final model we submitted\n",
    "\n",
    "plt.plot(regs,[t[0] for t in test_accs])\n",
    "print np.max([t[0] for t in test_accs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test dropout for the dense neural network. Devon also did a similar test. \n",
    "#The network architecture changed from above to here b/c Devon had a better version.\n",
    "\n",
    "dropouts = np.linspace(0.1,0.95,10)\n",
    "test_accs_dropout = []\n",
    "for drop in dropouts:\n",
    "    #Devon's code\n",
    "    reg = 0.000156\n",
    "\n",
    "    ## Create your own model here given the constraints in the problem\n",
    "    model = Sequential()\n",
    "    model.add(Dense(986, input_shape=(986,)))\n",
    "    #model.add(Dense(1000, input_shape=(1000,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(200,kernel_regularizer=regularizers.l1_l2(reg)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print 'Number of parameters = ',model.count_params()\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    #history = model.fit(X_train, y_train, epochs=6, batch_size=128, validation_data=(X_test, y_test))\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "    train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "    test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "    \n",
    "    test_accs_dropout.append((np.max(history.history['val_acc']),np.argmax(history.history['val_acc'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(dropouts,[t[0] for t in test_accs_dropout])\n",
    "print np.max([t[0] for t in test_accs_dropout])\n",
    "print dropouts[np.argmax([t[0] for t in test_accs_dropout])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing neural nets on the dataset with reduced dimensions (407 words instead of 1000)\n",
    "\n",
    "#Drop anything between 0.4 and 0.6 in sentiment\n",
    "#neutral_words = np.array(ordered_words)[np.where((sentiments<0.55)&(sentiments>0.45))[0]]\n",
    "common_words = np.array(ordered_words)[np.where((np.array(freq) > 3000)&(sentiments<0.55)&(sentiments > 0.45))]\n",
    "\n",
    "#sparse_train = train.drop(neutral_words,axis=1)\n",
    "#sparse_test = test.drop(neutral_words,axis=1)\n",
    "sparse_train = train.drop(common_words,axis=1)\n",
    "sparse_test = train.drop(common_words,axis=1)\n",
    "\n",
    "print np.shape(sparse_train)\n",
    "sparse_words = sparse_train.keys()\n",
    "sparse_words\n",
    "\n",
    "#X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(train,y,test_size=0.25,random_state = 47)]\n",
    "X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(sparse_train,y,test_size=0.25,random_state = 47)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Organizing the sentiment of words into a pandas table for easy reference\n",
    "\n",
    "sentiment_table = pd.DataFrame(word_sentiment)\n",
    "sentiment_table = sentiment_table.set_index(2)\n",
    "sentiment_table.loc['refund'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters =  1202201\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 4s 242us/step - loss: 2.4873 - acc: 0.7685 - val_loss: 1.2577 - val_acc: 0.8208\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 3s 230us/step - loss: 0.8447 - acc: 0.8509 - val_loss: 0.6726 - val_acc: 0.8268\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 3s 227us/step - loss: 0.5173 - acc: 0.8752 - val_loss: 0.5516 - val_acc: 0.8398\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 3s 230us/step - loss: 0.4245 - acc: 0.8962 - val_loss: 0.5575 - val_acc: 0.8468\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 5s 326us/step - loss: 0.3654 - acc: 0.9255 - val_loss: 0.5583 - val_acc: 0.8492\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 3s 227us/step - loss: 0.3140 - acc: 0.9402 - val_loss: 0.7206 - val_acc: 0.8156\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 3s 230us/step - loss: 0.2640 - acc: 0.9609 - val_loss: 0.8062 - val_acc: 0.8230\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 240us/step - loss: 0.2246 - acc: 0.9704 - val_loss: 0.6681 - val_acc: 0.8464\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 236us/step - loss: 0.1858 - acc: 0.9805 - val_loss: 0.9048 - val_acc: 0.7944\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 3s 228us/step - loss: 0.1627 - acc: 0.9844 - val_loss: 0.7107 - val_acc: 0.8444\n",
      "15000/15000 [==============================] - 2s 146us/step\n",
      "5000/5000 [==============================] - 1s 142us/step\n"
     ]
    }
   ],
   "source": [
    "#One more round at regular neural net - this one didn't do too great\n",
    "\n",
    "\n",
    "## Create your own model here given the constraints in the problem\n",
    "model = Sequential()\n",
    "#model.add(Dense(986, input_shape=(986,)))\n",
    "model.add(Dense(1000, input_shape=(1000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(200,kernel_regularizer=regularizers.l1_l2(reg)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(drop))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "print 'Number of parameters = ',model.count_params()\n",
    "#model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#history = model.fit(X_train, y_train, epochs=6, batch_size=128, validation_data=(X_test, y_test))\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 4s 292us/step - loss: 0.4044 - acc: 0.8229 - val_loss: 0.3765 - val_acc: 0.8368\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 5s 361us/step - loss: 0.3545 - acc: 0.8477 - val_loss: 0.3496 - val_acc: 0.8518\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.3448 - acc: 0.8521 - val_loss: 0.3490 - val_acc: 0.8470\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3389 - acc: 0.8547 - val_loss: 0.3596 - val_acc: 0.8426\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3273 - acc: 0.8601 - val_loss: 0.3344 - val_acc: 0.8562\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3183 - acc: 0.8607 - val_loss: 0.3512 - val_acc: 0.8484\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.3159 - acc: 0.8648 - val_loss: 0.3533 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 282us/step - loss: 0.3092 - acc: 0.8677 - val_loss: 0.3288 - val_acc: 0.8582\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 293us/step - loss: 0.2997 - acc: 0.8724 - val_loss: 0.3405 - val_acc: 0.8492\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 6s 384us/step - loss: 0.2907 - acc: 0.8770 - val_loss: 0.3488 - val_acc: 0.8512\n",
      "15000/15000 [==============================] - 2s 155us/step\n",
      "5000/5000 [==============================] - 1s 150us/step\n",
      "0.885999999968 0.8512\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 306us/step - loss: 0.4074 - acc: 0.8242 - val_loss: 0.4027 - val_acc: 0.8246\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3599 - acc: 0.8450 - val_loss: 0.3518 - val_acc: 0.8520\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 283us/step - loss: 0.3504 - acc: 0.8507 - val_loss: 0.3484 - val_acc: 0.8494\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3474 - acc: 0.8537 - val_loss: 0.4172 - val_acc: 0.8170\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 277us/step - loss: 0.3407 - acc: 0.8563 - val_loss: 0.3559 - val_acc: 0.8538\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 262us/step - loss: 0.3400 - acc: 0.8528 - val_loss: 0.3528 - val_acc: 0.8484\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 5s 356us/step - loss: 0.3323 - acc: 0.8567 - val_loss: 0.3576 - val_acc: 0.8500\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 279us/step - loss: 0.3296 - acc: 0.8581 - val_loss: 0.3391 - val_acc: 0.8544\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3262 - acc: 0.8597 - val_loss: 0.3389 - val_acc: 0.8532\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 263us/step - loss: 0.3200 - acc: 0.8612 - val_loss: 0.3388 - val_acc: 0.8516\n",
      "15000/15000 [==============================] - 2s 148us/step\n",
      "5000/5000 [==============================] - 1s 152us/step\n",
      "0.8748 0.8516\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 316us/step - loss: 0.4099 - acc: 0.8187 - val_loss: 0.3597 - val_acc: 0.8536\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 297us/step - loss: 0.3626 - acc: 0.8473 - val_loss: 0.3561 - val_acc: 0.8456\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 298us/step - loss: 0.3546 - acc: 0.8475 - val_loss: 0.3486 - val_acc: 0.8554\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 6s 385us/step - loss: 0.3469 - acc: 0.8527 - val_loss: 0.3529 - val_acc: 0.8444\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 291us/step - loss: 0.3435 - acc: 0.8505 - val_loss: 0.3447 - val_acc: 0.8542\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 287us/step - loss: 0.3354 - acc: 0.8540 - val_loss: 0.3766 - val_acc: 0.8364\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 262us/step - loss: 0.3284 - acc: 0.8579 - val_loss: 0.3467 - val_acc: 0.8492\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 288us/step - loss: 0.3240 - acc: 0.8603 - val_loss: 0.3367 - val_acc: 0.8532\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 286us/step - loss: 0.3186 - acc: 0.8633 - val_loss: 0.3385 - val_acc: 0.8530\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 281us/step - loss: 0.3124 - acc: 0.8661 - val_loss: 0.3342 - val_acc: 0.8538\n",
      "15000/15000 [==============================] - 2s 144us/step\n",
      "5000/5000 [==============================] - 1s 169us/step\n",
      "0.882533333302 0.8538\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 6s 397us/step - loss: 0.4133 - acc: 0.8184 - val_loss: 0.3794 - val_acc: 0.8416\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 254us/step - loss: 0.3630 - acc: 0.8424 - val_loss: 0.3594 - val_acc: 0.8494\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3545 - acc: 0.8438 - val_loss: 0.3636 - val_acc: 0.8430\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3439 - acc: 0.8511 - val_loss: 0.3454 - val_acc: 0.8528\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 283us/step - loss: 0.3372 - acc: 0.8555 - val_loss: 0.3412 - val_acc: 0.8562\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 268us/step - loss: 0.3313 - acc: 0.8546 - val_loss: 0.3385 - val_acc: 0.8542\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.3272 - acc: 0.8577 - val_loss: 0.3555 - val_acc: 0.8402\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 5s 360us/step - loss: 0.3185 - acc: 0.8625 - val_loss: 0.3381 - val_acc: 0.8564\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 261us/step - loss: 0.3146 - acc: 0.8664 - val_loss: 0.3601 - val_acc: 0.8434\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 260us/step - loss: 0.3065 - acc: 0.8683 - val_loss: 0.3403 - val_acc: 0.8570\n",
      "15000/15000 [==============================] - 2s 163us/step\n",
      "5000/5000 [==============================] - 1s 155us/step\n",
      "0.886933333333 0.857\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 4s 295us/step - loss: 0.4154 - acc: 0.8203 - val_loss: 0.3657 - val_acc: 0.8486\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3627 - acc: 0.8433 - val_loss: 0.3559 - val_acc: 0.8462\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 264us/step - loss: 0.3584 - acc: 0.8458 - val_loss: 0.3545 - val_acc: 0.8524\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3519 - acc: 0.8493 - val_loss: 0.3471 - val_acc: 0.8500\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 6s 382us/step - loss: 0.3482 - acc: 0.8495 - val_loss: 0.3435 - val_acc: 0.8510\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3441 - acc: 0.8531 - val_loss: 0.3528 - val_acc: 0.8466\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 268us/step - loss: 0.3405 - acc: 0.8533 - val_loss: 0.3390 - val_acc: 0.8544\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 272us/step - loss: 0.3375 - acc: 0.8535 - val_loss: 0.3439 - val_acc: 0.8490\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 263us/step - loss: 0.3319 - acc: 0.8547 - val_loss: 0.3357 - val_acc: 0.8510\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 259us/step - loss: 0.3310 - acc: 0.8553 - val_loss: 0.3425 - val_acc: 0.8464\n",
      "15000/15000 [==============================] - 2s 155us/step\n",
      "5000/5000 [==============================] - 1s 149us/step\n",
      "0.868466666635 0.8464\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 313us/step - loss: 0.4220 - acc: 0.8132 - val_loss: 0.3629 - val_acc: 0.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 5s 357us/step - loss: 0.3728 - acc: 0.8414 - val_loss: 0.3552 - val_acc: 0.8496\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.3638 - acc: 0.8439 - val_loss: 0.3627 - val_acc: 0.8494\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 268us/step - loss: 0.3627 - acc: 0.8476 - val_loss: 0.3516 - val_acc: 0.8506\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3614 - acc: 0.8473 - val_loss: 0.3538 - val_acc: 0.8512\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.3577 - acc: 0.8484 - val_loss: 0.3633 - val_acc: 0.8462\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3539 - acc: 0.8457 - val_loss: 0.3509 - val_acc: 0.8518\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 290us/step - loss: 0.3514 - acc: 0.8490 - val_loss: 0.3510 - val_acc: 0.8502\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 275us/step - loss: 0.3460 - acc: 0.8501 - val_loss: 0.3462 - val_acc: 0.8546\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 5s 360us/step - loss: 0.3488 - acc: 0.8481 - val_loss: 0.3599 - val_acc: 0.8480\n",
      "15000/15000 [==============================] - 2s 154us/step\n",
      "5000/5000 [==============================] - 1s 158us/step\n",
      "0.854733333302 0.848\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 336us/step - loss: 0.4209 - acc: 0.8145 - val_loss: 0.3785 - val_acc: 0.8384\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 285us/step - loss: 0.3734 - acc: 0.8381 - val_loss: 0.3790 - val_acc: 0.8416\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 293us/step - loss: 0.3649 - acc: 0.8439 - val_loss: 0.3561 - val_acc: 0.8474\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 297us/step - loss: 0.3572 - acc: 0.8431 - val_loss: 0.3450 - val_acc: 0.8518\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 5s 328us/step - loss: 0.3551 - acc: 0.8478 - val_loss: 0.3449 - val_acc: 0.8532\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 282us/step - loss: 0.3518 - acc: 0.8481 - val_loss: 0.3437 - val_acc: 0.8536\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 5s 362us/step - loss: 0.3449 - acc: 0.8485 - val_loss: 0.3431 - val_acc: 0.8504\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3453 - acc: 0.8483 - val_loss: 0.3445 - val_acc: 0.8552\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 278us/step - loss: 0.3407 - acc: 0.8535 - val_loss: 0.3370 - val_acc: 0.8540\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.3361 - acc: 0.8533 - val_loss: 0.3490 - val_acc: 0.8556\n",
      "15000/15000 [==============================] - 2s 151us/step\n",
      "5000/5000 [==============================] - 1s 158us/step\n",
      "0.867533333365 0.8556\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 315us/step - loss: 0.4285 - acc: 0.8107 - val_loss: 0.3945 - val_acc: 0.8278\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 283us/step - loss: 0.3781 - acc: 0.8383 - val_loss: 0.3982 - val_acc: 0.8384\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 272us/step - loss: 0.3698 - acc: 0.8405 - val_loss: 0.3681 - val_acc: 0.8476\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 6s 367us/step - loss: 0.3688 - acc: 0.8406 - val_loss: 0.3535 - val_acc: 0.8506\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 279us/step - loss: 0.3628 - acc: 0.8437 - val_loss: 0.3491 - val_acc: 0.8498\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3545 - acc: 0.8448 - val_loss: 0.3431 - val_acc: 0.8506\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 281us/step - loss: 0.3531 - acc: 0.8442 - val_loss: 0.3430 - val_acc: 0.8504\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.3470 - acc: 0.8495 - val_loss: 0.3466 - val_acc: 0.8538\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 278us/step - loss: 0.3452 - acc: 0.8487 - val_loss: 0.3453 - val_acc: 0.8552\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 278us/step - loss: 0.3406 - acc: 0.8548 - val_loss: 0.3384 - val_acc: 0.8542\n",
      "15000/15000 [==============================] - 2s 149us/step\n",
      "5000/5000 [==============================] - 1s 150us/step\n",
      "0.870266666667 0.8542\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 312us/step - loss: 0.4317 - acc: 0.8112 - val_loss: 0.3596 - val_acc: 0.8536\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3810 - acc: 0.8359 - val_loss: 0.3528 - val_acc: 0.8542\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 281us/step - loss: 0.3705 - acc: 0.8421 - val_loss: 0.3562 - val_acc: 0.8546\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 274us/step - loss: 0.3685 - acc: 0.8414 - val_loss: 0.3522 - val_acc: 0.8540\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.3609 - acc: 0.8423 - val_loss: 0.3502 - val_acc: 0.8516\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3533 - acc: 0.8476 - val_loss: 0.3460 - val_acc: 0.8564\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.3555 - acc: 0.8460 - val_loss: 0.3438 - val_acc: 0.8522\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 6s 367us/step - loss: 0.3542 - acc: 0.8455 - val_loss: 0.3438 - val_acc: 0.8558\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3470 - acc: 0.8485 - val_loss: 0.3431 - val_acc: 0.8564\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 274us/step - loss: 0.3497 - acc: 0.8471 - val_loss: 0.3432 - val_acc: 0.8538\n",
      "15000/15000 [==============================] - 2s 156us/step\n",
      "5000/5000 [==============================] - 1s 154us/step\n",
      "0.863866666667 0.8538\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 315us/step - loss: 0.4350 - acc: 0.8075 - val_loss: 0.3729 - val_acc: 0.8482\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 276us/step - loss: 0.3864 - acc: 0.8333 - val_loss: 0.3825 - val_acc: 0.8422\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 272us/step - loss: 0.3820 - acc: 0.8347 - val_loss: 0.3660 - val_acc: 0.8434\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 272us/step - loss: 0.3693 - acc: 0.8420 - val_loss: 0.3737 - val_acc: 0.8474\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 5s 358us/step - loss: 0.3648 - acc: 0.8419 - val_loss: 0.3600 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 276us/step - loss: 0.3654 - acc: 0.8387 - val_loss: 0.3570 - val_acc: 0.8496\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 278us/step - loss: 0.3611 - acc: 0.8379 - val_loss: 0.3631 - val_acc: 0.8474\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.3549 - acc: 0.8431 - val_loss: 0.3508 - val_acc: 0.8516\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3585 - acc: 0.8457 - val_loss: 0.3500 - val_acc: 0.8506\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.3554 - acc: 0.8442 - val_loss: 0.3445 - val_acc: 0.8490\n",
      "15000/15000 [==============================] - 2s 149us/step\n",
      "5000/5000 [==============================] - 1s 135us/step\n",
      "0.864066666667 0.849\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 312us/step - loss: 0.4554 - acc: 0.7955 - val_loss: 0.3721 - val_acc: 0.8490\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 5s 364us/step - loss: 0.3935 - acc: 0.8316 - val_loss: 0.3682 - val_acc: 0.8520\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 261us/step - loss: 0.3894 - acc: 0.8325 - val_loss: 0.3644 - val_acc: 0.8502\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3790 - acc: 0.8368 - val_loss: 0.3594 - val_acc: 0.8496\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 261us/step - loss: 0.3749 - acc: 0.8375 - val_loss: 0.3674 - val_acc: 0.8466\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 261us/step - loss: 0.3700 - acc: 0.8397 - val_loss: 0.3518 - val_acc: 0.8516\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3703 - acc: 0.8356 - val_loss: 0.3660 - val_acc: 0.8504\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.3647 - acc: 0.8424 - val_loss: 0.3608 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 263us/step - loss: 0.3610 - acc: 0.8422 - val_loss: 0.3452 - val_acc: 0.8488\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 5s 362us/step - loss: 0.3613 - acc: 0.8417 - val_loss: 0.3461 - val_acc: 0.8518\n",
      "15000/15000 [==============================] - 2s 160us/step\n",
      "5000/5000 [==============================] - 1s 142us/step\n",
      "0.864133333333 0.8518\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 310us/step - loss: 0.4540 - acc: 0.7950 - val_loss: 0.3792 - val_acc: 0.8460\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.4033 - acc: 0.8256 - val_loss: 0.3784 - val_acc: 0.8462\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.3898 - acc: 0.8307 - val_loss: 0.3696 - val_acc: 0.8506\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3849 - acc: 0.8297 - val_loss: 0.3666 - val_acc: 0.8408\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 264us/step - loss: 0.3787 - acc: 0.8319 - val_loss: 0.3526 - val_acc: 0.8504\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3703 - acc: 0.8349 - val_loss: 0.3448 - val_acc: 0.8524\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 5s 366us/step - loss: 0.3681 - acc: 0.8357 - val_loss: 0.3551 - val_acc: 0.8514\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 261us/step - loss: 0.3710 - acc: 0.8317 - val_loss: 0.3603 - val_acc: 0.8522\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 260us/step - loss: 0.3668 - acc: 0.8400 - val_loss: 0.3887 - val_acc: 0.8492\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 263us/step - loss: 0.3657 - acc: 0.8391 - val_loss: 0.3571 - val_acc: 0.8532\n",
      "15000/15000 [==============================] - 2s 156us/step\n",
      "5000/5000 [==============================] - 1s 162us/step\n",
      "0.8634 0.8532\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 319us/step - loss: 0.4613 - acc: 0.7911 - val_loss: 0.3952 - val_acc: 0.8402\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 268us/step - loss: 0.4050 - acc: 0.8241 - val_loss: 0.3881 - val_acc: 0.8468\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3963 - acc: 0.8260 - val_loss: 0.3910 - val_acc: 0.8462\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 5s 353us/step - loss: 0.3926 - acc: 0.8282 - val_loss: 0.3992 - val_acc: 0.8484\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.3856 - acc: 0.8329 - val_loss: 0.3919 - val_acc: 0.8510\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 264us/step - loss: 0.3862 - acc: 0.8318 - val_loss: 0.3884 - val_acc: 0.8500\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 285us/step - loss: 0.3846 - acc: 0.8311 - val_loss: 0.3801 - val_acc: 0.8516\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 275us/step - loss: 0.3833 - acc: 0.8333 - val_loss: 0.3949 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.3716 - acc: 0.8365 - val_loss: 0.3814 - val_acc: 0.8530\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.3746 - acc: 0.8353 - val_loss: 0.3894 - val_acc: 0.8500\n",
      "15000/15000 [==============================] - 2s 151us/step\n",
      "5000/5000 [==============================] - 1s 146us/step\n",
      "0.860133333302 0.85\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 6s 408us/step - loss: 0.4730 - acc: 0.7850 - val_loss: 0.3834 - val_acc: 0.8456\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.4175 - acc: 0.8179 - val_loss: 0.3869 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 262us/step - loss: 0.4049 - acc: 0.8232 - val_loss: 0.3891 - val_acc: 0.8440\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 268us/step - loss: 0.3966 - acc: 0.8259 - val_loss: 0.3710 - val_acc: 0.8526\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 259us/step - loss: 0.3914 - acc: 0.8256 - val_loss: 0.3838 - val_acc: 0.8504\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.3873 - acc: 0.8299 - val_loss: 0.3850 - val_acc: 0.8526\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.3863 - acc: 0.8263 - val_loss: 0.3813 - val_acc: 0.8526\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 263us/step - loss: 0.3846 - acc: 0.8304 - val_loss: 0.3522 - val_acc: 0.8504\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 5s 357us/step - loss: 0.3847 - acc: 0.8290 - val_loss: 0.3597 - val_acc: 0.8490\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.3733 - acc: 0.8329 - val_loss: 0.3531 - val_acc: 0.8520\n",
      "15000/15000 [==============================] - 2s 144us/step\n",
      "5000/5000 [==============================] - 1s 151us/step\n",
      "0.857799999968 0.852\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 340us/step - loss: 0.4972 - acc: 0.7635 - val_loss: 0.3965 - val_acc: 0.8458\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 282us/step - loss: 0.4388 - acc: 0.8063 - val_loss: 0.3997 - val_acc: 0.8372\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 279us/step - loss: 0.4254 - acc: 0.8149 - val_loss: 0.3953 - val_acc: 0.8490\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 277us/step - loss: 0.4151 - acc: 0.8217 - val_loss: 0.3987 - val_acc: 0.8452\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.4208 - acc: 0.8135 - val_loss: 0.4012 - val_acc: 0.8498\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 5s 364us/step - loss: 0.4089 - acc: 0.8192 - val_loss: 0.4035 - val_acc: 0.8484\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4091 - acc: 0.8185 - val_loss: 0.4058 - val_acc: 0.8434\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.4048 - acc: 0.8208 - val_loss: 0.4094 - val_acc: 0.8458\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4041 - acc: 0.8231 - val_loss: 0.4077 - val_acc: 0.8428\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 272us/step - loss: 0.4004 - acc: 0.8231 - val_loss: 0.4033 - val_acc: 0.8444\n",
      "15000/15000 [==============================] - 2s 154us/step\n",
      "5000/5000 [==============================] - 1s 138us/step\n",
      "0.852200000032 0.8444\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 344us/step - loss: 0.5116 - acc: 0.7577 - val_loss: 0.4396 - val_acc: 0.8284\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 276us/step - loss: 0.4496 - acc: 0.7974 - val_loss: 0.4307 - val_acc: 0.8502\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 5s 352us/step - loss: 0.4402 - acc: 0.8052 - val_loss: 0.4395 - val_acc: 0.8392\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4319 - acc: 0.8080 - val_loss: 0.4498 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 274us/step - loss: 0.4270 - acc: 0.8107 - val_loss: 0.4498 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4209 - acc: 0.8123 - val_loss: 0.4529 - val_acc: 0.8392\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4180 - acc: 0.8139 - val_loss: 0.4417 - val_acc: 0.8428\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 259us/step - loss: 0.4114 - acc: 0.8197 - val_loss: 0.4385 - val_acc: 0.8462\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.4122 - acc: 0.8189 - val_loss: 0.4329 - val_acc: 0.8468\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.4099 - acc: 0.8145 - val_loss: 0.4135 - val_acc: 0.8456\n",
      "15000/15000 [==============================] - 4s 245us/step\n",
      "5000/5000 [==============================] - 1s 155us/step\n",
      "0.851066666635 0.8456\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 341us/step - loss: 0.5297 - acc: 0.7415 - val_loss: 0.4500 - val_acc: 0.8292\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 282us/step - loss: 0.4592 - acc: 0.7910 - val_loss: 0.4430 - val_acc: 0.8270\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.4488 - acc: 0.7987 - val_loss: 0.4539 - val_acc: 0.8310\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.4424 - acc: 0.8035 - val_loss: 0.4427 - val_acc: 0.8478\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4361 - acc: 0.8001 - val_loss: 0.4349 - val_acc: 0.8432\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.4344 - acc: 0.8071 - val_loss: 0.4439 - val_acc: 0.8382\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 5s 365us/step - loss: 0.4260 - acc: 0.8109 - val_loss: 0.4336 - val_acc: 0.8380\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.4261 - acc: 0.8115 - val_loss: 0.4457 - val_acc: 0.8388\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.4216 - acc: 0.8125 - val_loss: 0.4306 - val_acc: 0.8420\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 275us/step - loss: 0.4229 - acc: 0.8123 - val_loss: 0.4425 - val_acc: 0.8412\n",
      "15000/15000 [==============================] - 2s 159us/step\n",
      "5000/5000 [==============================] - 1s 151us/step\n",
      "0.849333333302 0.8412\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 349us/step - loss: 0.5795 - acc: 0.6997 - val_loss: 0.5283 - val_acc: 0.8232\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 292us/step - loss: 0.5093 - acc: 0.7607 - val_loss: 0.5234 - val_acc: 0.8368\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 276us/step - loss: 0.4992 - acc: 0.7725 - val_loss: 0.5047 - val_acc: 0.8436\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 5s 364us/step - loss: 0.4858 - acc: 0.7743 - val_loss: 0.5142 - val_acc: 0.8382\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.4802 - acc: 0.7796 - val_loss: 0.4901 - val_acc: 0.8344\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 279us/step - loss: 0.4799 - acc: 0.7775 - val_loss: 0.4995 - val_acc: 0.8366\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 280us/step - loss: 0.4773 - acc: 0.7833 - val_loss: 0.4945 - val_acc: 0.8380\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 273us/step - loss: 0.4704 - acc: 0.7835 - val_loss: 0.4898 - val_acc: 0.8452\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.4735 - acc: 0.7831 - val_loss: 0.4856 - val_acc: 0.8352\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.4660 - acc: 0.7879 - val_loss: 0.4718 - val_acc: 0.8382\n",
      "15000/15000 [==============================] - 2s 158us/step\n",
      "5000/5000 [==============================] - 1s 147us/step\n",
      "0.842200000032 0.8382\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 7s 436us/step - loss: 0.6535 - acc: 0.6219 - val_loss: 0.6116 - val_acc: 0.8262\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 265us/step - loss: 0.5853 - acc: 0.7003 - val_loss: 0.5774 - val_acc: 0.8340\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.5614 - acc: 0.7177 - val_loss: 0.5709 - val_acc: 0.8328\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.5432 - acc: 0.7331 - val_loss: 0.5542 - val_acc: 0.8264\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.5415 - acc: 0.7382 - val_loss: 0.5646 - val_acc: 0.8146\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 4s 267us/step - loss: 0.5358 - acc: 0.7390 - val_loss: 0.5455 - val_acc: 0.8158\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 275us/step - loss: 0.5303 - acc: 0.7419 - val_loss: 0.5593 - val_acc: 0.8148\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 263us/step - loss: 0.5272 - acc: 0.7485 - val_loss: 0.5444 - val_acc: 0.8298\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 5s 353us/step - loss: 0.5208 - acc: 0.7523 - val_loss: 0.5425 - val_acc: 0.8270\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 274us/step - loss: 0.5255 - acc: 0.7483 - val_loss: 0.5462 - val_acc: 0.8018\n",
      "15000/15000 [==============================] - 2s 150us/step\n",
      "5000/5000 [==============================] - 1s 142us/step\n",
      "0.803133333302 0.8018\n",
      "Number of parameters =  48481\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 5s 344us/step - loss: 0.7354 - acc: 0.5629 - val_loss: 0.6806 - val_acc: 0.7570\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 4s 269us/step - loss: 0.6553 - acc: 0.6230 - val_loss: 0.6525 - val_acc: 0.7784\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 4s 275us/step - loss: 0.6352 - acc: 0.6431 - val_loss: 0.6315 - val_acc: 0.7696\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 4s 270us/step - loss: 0.6172 - acc: 0.6640 - val_loss: 0.6035 - val_acc: 0.8064\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 4s 271us/step - loss: 0.6183 - acc: 0.6675 - val_loss: 0.6007 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 5s 362us/step - loss: 0.6172 - acc: 0.6689 - val_loss: 0.6117 - val_acc: 0.7640\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 4s 274us/step - loss: 0.6122 - acc: 0.6703 - val_loss: 0.6006 - val_acc: 0.7804\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 4s 266us/step - loss: 0.6045 - acc: 0.6771 - val_loss: 0.6122 - val_acc: 0.7156\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 4s 268us/step - loss: 0.6049 - acc: 0.6747 - val_loss: 0.6114 - val_acc: 0.7428\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 4s 272us/step - loss: 0.5994 - acc: 0.6839 - val_loss: 0.6198 - val_acc: 0.7290\n",
      "15000/15000 [==============================] - 2s 153us/step\n",
      "5000/5000 [==============================] - 1s 155us/step\n",
      "0.736066666667 0.729\n"
     ]
    }
   ],
   "source": [
    "#Try a CNN...\n",
    "\n",
    "#Reshuffle the data columns\n",
    "sorted_train = np.array(train[np.array(sentiment_table.index)])\n",
    "sorted_test = np.array(test[np.array(sentiment_table.index)])\n",
    "\n",
    "X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(sorted_train,y,test_size=0.25,random_state = 47)]\n",
    "\n",
    "# we must reshape the X data (add a channel dimension)\n",
    "X_train = X_train.reshape(tuple(list(X_train.shape)+[1]))\n",
    "X_test = X_test.reshape(tuple(list(X_test.shape)+[1]))\n",
    "\n",
    "#print np.shape(X_train)\n",
    "\n",
    "\n",
    "dropouts = np.linspace(0.1,0.95,20)\n",
    "test_accs_dropout = []\n",
    "for drop in dropouts:    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(8, kernel_size=3, padding='same',input_shape=(1000,1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Conv1D(8, kernel_size=3, padding='same'))\n",
    "    model.add(Conv1D(8, kernel_size=3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(24))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print 'Number of parameters = ',model.count_params()\n",
    "\n",
    "    model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "    train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "    test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "    print train_acc, test_acc\n",
    "    \n",
    "    test_accs_dropout.append((np.max(history.history['val_acc']),np.argmax(history.history['val_acc'])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8582\n",
      "0.1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f259d07a750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dropouts,[t[0] for t in test_accs_dropout])\n",
    "print np.max([t[0] for t in test_accs_dropout])\n",
    "print dropouts[np.argmax([t[0] for t in test_accs_dropout])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now try a CNN with one hot encoding so we can use Dense(2) at the end and softmax\n",
    "\n",
    "sorted_train = np.array(train[np.array(sentiment_table.index)])\n",
    "sorted_test = np.array(test[np.array(sentiment_table.index)])\n",
    "\n",
    "X_train,X_test,y_train,y_test = [np.array(df) for df in train_test_split(sorted_train,y,test_size=0.25,random_state = 47)]\n",
    "\n",
    "# we must reshape the X data (add a channel dimension)\n",
    "X_train = X_train.reshape(tuple(list(X_train.shape)+[1]))\n",
    "X_test = X_test.reshape(tuple(list(X_test.shape)+[1]))\n",
    "\n",
    "# we'll need to one-hot encode the labels\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters =  202510\n",
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 14s 936us/step - loss: 0.4150 - acc: 0.8219 - val_loss: 0.3482 - val_acc: 0.8500\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 13s 877us/step - loss: 0.3451 - acc: 0.8531 - val_loss: 0.3417 - val_acc: 0.8480\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 11s 759us/step - loss: 0.3317 - acc: 0.8561 - val_loss: 0.3326 - val_acc: 0.8568\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 12s 773us/step - loss: 0.3232 - acc: 0.8603 - val_loss: 0.3308 - val_acc: 0.8536\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 13s 868us/step - loss: 0.3143 - acc: 0.8677 - val_loss: 0.3326 - val_acc: 0.8548\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 12s 784us/step - loss: 0.3071 - acc: 0.8686 - val_loss: 0.3316 - val_acc: 0.8572\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 13s 870us/step - loss: 0.2970 - acc: 0.8746 - val_loss: 0.3339 - val_acc: 0.8566\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 11s 763us/step - loss: 0.2963 - acc: 0.8757 - val_loss: 0.3329 - val_acc: 0.8592\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 11s 766us/step - loss: 0.2880 - acc: 0.8811 - val_loss: 0.3309 - val_acc: 0.8564\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 13s 858us/step - loss: 0.2812 - acc: 0.8833 - val_loss: 0.3371 - val_acc: 0.8534\n",
      "15000/15000 [==============================] - 4s 285us/step\n",
      "5000/5000 [==============================] - 1s 281us/step\n",
      "0.908866666667 0.8534\n"
     ]
    }
   ],
   "source": [
    "#The code for the CNN. \n",
    "ksize = 5\n",
    "mpool = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=ksize, padding='same',input_shape=(1000,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=mpool))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(8, kernel_size=ksize, padding='same'))\n",
    "model.add(Conv1D(8, kernel_size=ksize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=mpool))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print 'Number of parameters = ',model.count_params()\n",
    "\n",
    "model.compile(optimizer='adagrad',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "print train_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wrapping up the CNN in a function for easier testing. \n",
    "#We'd use this function for a gridsearch if we had more time\n",
    "\n",
    "def train_cnn(X_train,y_train,X_test,y_test,ksize=5,mpool=2,nepochs=5):\n",
    "    \n",
    "    # we must reshape the X data (add a channel dimension)\n",
    "    X_train = X_train.reshape(tuple(list(X_train.shape)+[1]))\n",
    "    X_test = X_test.reshape(tuple(list(X_test.shape)+[1]))\n",
    "    \n",
    "    # we'll need to one-hot encode the labels\n",
    "    y_train = keras.utils.np_utils.to_categorical(y_train)\n",
    "    y_test = keras.utils.np_utils.to_categorical(y_test)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, kernel_size=ksize, padding='same',input_shape=(1000,1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=mpool))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv1D(8, kernel_size=ksize, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=mpool))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print 'Number of parameters = ',model.count_params()\n",
    "\n",
    "    model.compile(optimizer='adagrad',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=nepochs, batch_size=128, validation_data=(X_test, y_test))\n",
    "    train_loss, train_acc = model.evaluate(x=X_train, y=y_train)\n",
    "    test_loss, test_acc = model.evaluate(x=X_test, y=y_test)\n",
    "\n",
    "    return (model,train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d_91 (None, 1000, 1) (None, 1000, 32)\n",
      "activation_127 (None, 1000, 32) (None, 1000, 32)\n",
      "max_pooling1d_61 (None, 1000, 32) (None, 500, 32)\n",
      "dropout_59 (None, 500, 32) (None, 500, 32)\n",
      "conv1d_92 (None, 500, 32) (None, 500, 8)\n",
      "conv1d_93 (None, 500, 8) (None, 500, 8)\n",
      "activation_128 (None, 500, 8) (None, 500, 8)\n",
      "max_pooling1d_62 (None, 500, 8) (None, 250, 8)\n",
      "dropout_60 (None, 250, 8) (None, 250, 8)\n",
      "flatten_31 (None, 250, 8) (None, 2000)\n",
      "dense_67 (None, 2000) (None, 24)\n",
      "activation_129 (None, 24) (None, 24)\n",
      "dense_68 (None, 24) (None, 1)\n",
      "activation_130 (None, 1) (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print layer.name, layer.input_shape, layer.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 15s 802us/step - loss: 0.4059 - acc: 0.8254 - val_loss: 0.3109 - val_acc: 0.8671\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 13s 698us/step - loss: 0.3471 - acc: 0.8493 - val_loss: 0.3008 - val_acc: 0.8681\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 15s 775us/step - loss: 0.3318 - acc: 0.8584 - val_loss: 0.2970 - val_acc: 0.8681\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 13s 700us/step - loss: 0.3219 - acc: 0.8620 - val_loss: 0.2926 - val_acc: 0.8671\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 15s 790us/step - loss: 0.3159 - acc: 0.8637 - val_loss: 0.2927 - val_acc: 0.8651\n",
      "18999/18999 [==============================] - 5s 250us/step\n",
      "1001/1001 [==============================] - 0s 248us/step\n",
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 23s 1ms/step - loss: 0.3977 - acc: 0.8262 - val_loss: 0.3166 - val_acc: 0.8721\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 15s 769us/step - loss: 0.3470 - acc: 0.8525 - val_loss: 0.3081 - val_acc: 0.8661\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 13s 690us/step - loss: 0.3317 - acc: 0.8543 - val_loss: 0.3081 - val_acc: 0.8671\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 15s 779us/step - loss: 0.3260 - acc: 0.8585 - val_loss: 0.3043 - val_acc: 0.8721\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 13s 696us/step - loss: 0.3129 - acc: 0.8666 - val_loss: 0.3043 - val_acc: 0.8681\n",
      "18999/18999 [==============================] - 5s 248us/step\n",
      "1001/1001 [==============================] - 0s 244us/step\n",
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 16s 823us/step - loss: 0.4037 - acc: 0.8260 - val_loss: 0.3217 - val_acc: 0.8761\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 13s 709us/step - loss: 0.3488 - acc: 0.8498 - val_loss: 0.3250 - val_acc: 0.8611\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 15s 771us/step - loss: 0.3388 - acc: 0.8538 - val_loss: 0.3236 - val_acc: 0.8621\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 13s 696us/step - loss: 0.3273 - acc: 0.8588 - val_loss: 0.3226 - val_acc: 0.8731\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 13s 699us/step - loss: 0.3196 - acc: 0.8642 - val_loss: 0.3268 - val_acc: 0.8581\n",
      "18999/18999 [==============================] - 6s 322us/step\n",
      "1001/1001 [==============================] - 0s 251us/step\n",
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 14s 736us/step - loss: 0.3969 - acc: 0.8248 - val_loss: 0.3567 - val_acc: 0.8501\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 15s 781us/step - loss: 0.3406 - acc: 0.8512 - val_loss: 0.3565 - val_acc: 0.8521\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 13s 704us/step - loss: 0.3289 - acc: 0.8583 - val_loss: 0.3508 - val_acc: 0.8501\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 14s 713us/step - loss: 0.3178 - acc: 0.8652 - val_loss: 0.3498 - val_acc: 0.8501\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 15s 784us/step - loss: 0.3091 - acc: 0.8677 - val_loss: 0.3542 - val_acc: 0.8492\n",
      "18999/18999 [==============================] - 5s 255us/step\n",
      "1001/1001 [==============================] - 0s 246us/step\n",
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 14s 756us/step - loss: 0.4064 - acc: 0.8208 - val_loss: 0.3291 - val_acc: 0.8601\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 15s 774us/step - loss: 0.3534 - acc: 0.8458 - val_loss: 0.3262 - val_acc: 0.8561\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 14s 721us/step - loss: 0.3348 - acc: 0.8532 - val_loss: 0.3228 - val_acc: 0.8571\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 15s 783us/step - loss: 0.3264 - acc: 0.8588 - val_loss: 0.3234 - val_acc: 0.8541\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 14s 712us/step - loss: 0.3173 - acc: 0.8646 - val_loss: 0.3168 - val_acc: 0.8501\n",
      "18999/18999 [==============================] - 5s 257us/step\n",
      "1001/1001 [==============================] - 0s 256us/step\n",
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 15s 790us/step - loss: 0.4081 - acc: 0.8232 - val_loss: 0.3511 - val_acc: 0.8551\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 13s 673us/step - loss: 0.3514 - acc: 0.8474 - val_loss: 0.3347 - val_acc: 0.8641\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 14s 744us/step - loss: 0.3327 - acc: 0.8560 - val_loss: 0.3285 - val_acc: 0.8701\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 13s 665us/step - loss: 0.3229 - acc: 0.8600 - val_loss: 0.3272 - val_acc: 0.8711\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 12s 649us/step - loss: 0.3113 - acc: 0.8679 - val_loss: 0.3291 - val_acc: 0.8661\n",
      "18999/18999 [==============================] - 7s 343us/step\n",
      "1001/1001 [==============================] - 0s 272us/step\n",
      "Number of parameters =  202510\n",
      "Train on 18999 samples, validate on 1001 samples\n",
      "Epoch 1/5\n",
      "18999/18999 [==============================] - 13s 706us/step - loss: 0.4012 - acc: 0.8255 - val_loss: 0.3365 - val_acc: 0.8651\n",
      "Epoch 2/5\n",
      "18999/18999 [==============================] - 14s 738us/step - loss: 0.3438 - acc: 0.8498 - val_loss: 0.3235 - val_acc: 0.8711\n",
      "Epoch 3/5\n",
      "18999/18999 [==============================] - 14s 729us/step - loss: 0.3272 - acc: 0.8586 - val_loss: 0.3333 - val_acc: 0.8691\n",
      "Epoch 4/5\n",
      "18999/18999 [==============================] - 14s 727us/step - loss: 0.3223 - acc: 0.8593 - val_loss: 0.3245 - val_acc: 0.8761\n",
      "Epoch 5/5\n",
      "18999/18999 [==============================] - 16s 837us/step - loss: 0.3138 - acc: 0.8652 - val_loss: 0.3237 - val_acc: 0.8661\n",
      "18999/18999 [==============================] - 5s 269us/step\n",
      "1001/1001 [==============================] - 0s 268us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "19000/19000 [==============================] - 16s 842us/step - loss: 0.4102 - acc: 0.8216 - val_loss: 0.3478 - val_acc: 0.8530\n",
      "Epoch 2/5\n",
      "19000/19000 [==============================] - 14s 719us/step - loss: 0.3512 - acc: 0.8475 - val_loss: 0.3404 - val_acc: 0.8510\n",
      "Epoch 3/5\n",
      "19000/19000 [==============================] - 14s 724us/step - loss: 0.3338 - acc: 0.8567 - val_loss: 0.3549 - val_acc: 0.8270\n",
      "Epoch 4/5\n",
      "19000/19000 [==============================] - 15s 799us/step - loss: 0.3238 - acc: 0.8583 - val_loss: 0.3321 - val_acc: 0.8520\n",
      "Epoch 5/5\n",
      "19000/19000 [==============================] - 14s 732us/step - loss: 0.3218 - acc: 0.8642 - val_loss: 0.3346 - val_acc: 0.8430\n",
      "19000/19000 [==============================] - 5s 268us/step\n",
      "1000/1000 [==============================] - 0s 255us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "19000/19000 [==============================] - 15s 792us/step - loss: 0.3985 - acc: 0.8249 - val_loss: 0.3658 - val_acc: 0.8400\n",
      "Epoch 2/5\n",
      "19000/19000 [==============================] - 14s 726us/step - loss: 0.3401 - acc: 0.8538 - val_loss: 0.3626 - val_acc: 0.8330\n",
      "Epoch 3/5\n",
      "19000/19000 [==============================] - 15s 773us/step - loss: 0.3275 - acc: 0.8603 - val_loss: 0.3579 - val_acc: 0.8420\n",
      "Epoch 4/5\n",
      "19000/19000 [==============================] - 14s 737us/step - loss: 0.3167 - acc: 0.8641 - val_loss: 0.3516 - val_acc: 0.8390\n",
      "Epoch 5/5\n",
      "19000/19000 [==============================] - 15s 807us/step - loss: 0.3076 - acc: 0.8693 - val_loss: 0.3522 - val_acc: 0.8440\n",
      "19000/19000 [==============================] - 5s 269us/step\n",
      "1000/1000 [==============================] - 0s 276us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "19000/19000 [==============================] - 15s 800us/step - loss: 0.4014 - acc: 0.8245 - val_loss: 0.3582 - val_acc: 0.8390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "19000/19000 [==============================] - 15s 795us/step - loss: 0.3450 - acc: 0.8502 - val_loss: 0.3406 - val_acc: 0.8440\n",
      "Epoch 3/5\n",
      "19000/19000 [==============================] - 14s 715us/step - loss: 0.3304 - acc: 0.8570 - val_loss: 0.3355 - val_acc: 0.8470\n",
      "Epoch 4/5\n",
      "19000/19000 [==============================] - 15s 809us/step - loss: 0.3206 - acc: 0.8627 - val_loss: 0.3336 - val_acc: 0.8550\n",
      "Epoch 5/5\n",
      "19000/19000 [==============================] - 14s 744us/step - loss: 0.3107 - acc: 0.8689 - val_loss: 0.3346 - val_acc: 0.8490\n",
      "19000/19000 [==============================] - 5s 258us/step\n",
      "1000/1000 [==============================] - 0s 252us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "19000/19000 [==============================] - 16s 848us/step - loss: 0.4075 - acc: 0.8213 - val_loss: 0.3238 - val_acc: 0.8550\n",
      "Epoch 2/5\n",
      "19000/19000 [==============================] - 14s 757us/step - loss: 0.3452 - acc: 0.8502 - val_loss: 0.3184 - val_acc: 0.8610\n",
      "Epoch 3/5\n",
      "19000/19000 [==============================] - 15s 800us/step - loss: 0.3331 - acc: 0.8571 - val_loss: 0.3154 - val_acc: 0.8690\n",
      "Epoch 4/5\n",
      "19000/19000 [==============================] - 15s 779us/step - loss: 0.3201 - acc: 0.8626 - val_loss: 0.3131 - val_acc: 0.8650\n",
      "Epoch 5/5\n",
      "19000/19000 [==============================] - 16s 820us/step - loss: 0.3146 - acc: 0.8667 - val_loss: 0.3086 - val_acc: 0.8690\n",
      "19000/19000 [==============================] - 5s 264us/step\n",
      "1000/1000 [==============================] - 0s 258us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "19000/19000 [==============================] - 15s 770us/step - loss: 0.3987 - acc: 0.8271 - val_loss: 0.3731 - val_acc: 0.8360\n",
      "Epoch 2/5\n",
      "19000/19000 [==============================] - 15s 806us/step - loss: 0.3416 - acc: 0.8503 - val_loss: 0.3754 - val_acc: 0.8370\n",
      "Epoch 3/5\n",
      "19000/19000 [==============================] - 14s 716us/step - loss: 0.3262 - acc: 0.8575 - val_loss: 0.3611 - val_acc: 0.8350\n",
      "Epoch 4/5\n",
      "19000/19000 [==============================] - 15s 778us/step - loss: 0.3233 - acc: 0.8622 - val_loss: 0.3648 - val_acc: 0.8400\n",
      "Epoch 5/5\n",
      "19000/19000 [==============================] - 14s 719us/step - loss: 0.3121 - acc: 0.8662 - val_loss: 0.3642 - val_acc: 0.8340\n",
      "19000/19000 [==============================] - 5s 271us/step\n",
      "1000/1000 [==============================] - 0s 309us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "19000/19000 [==============================] - 17s 890us/step - loss: 0.4064 - acc: 0.8221 - val_loss: 0.3965 - val_acc: 0.8290\n",
      "Epoch 2/5\n",
      "19000/19000 [==============================] - 14s 723us/step - loss: 0.3485 - acc: 0.8495 - val_loss: 0.3613 - val_acc: 0.8380\n",
      "Epoch 3/5\n",
      "19000/19000 [==============================] - 16s 853us/step - loss: 0.3299 - acc: 0.8554 - val_loss: 0.3559 - val_acc: 0.8450\n",
      "Epoch 4/5\n",
      "19000/19000 [==============================] - 14s 745us/step - loss: 0.3244 - acc: 0.8616 - val_loss: 0.3547 - val_acc: 0.8470\n",
      "Epoch 5/5\n",
      "19000/19000 [==============================] - 16s 836us/step - loss: 0.3157 - acc: 0.8645 - val_loss: 0.3516 - val_acc: 0.8530\n",
      "19000/19000 [==============================] - 5s 264us/step\n",
      "1000/1000 [==============================] - 0s 280us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 15s 813us/step - loss: 0.4192 - acc: 0.8226 - val_loss: 0.3451 - val_acc: 0.8368\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 15s 789us/step - loss: 0.3513 - acc: 0.8485 - val_loss: 0.3437 - val_acc: 0.8539\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 14s 738us/step - loss: 0.3339 - acc: 0.8576 - val_loss: 0.3492 - val_acc: 0.8458\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 15s 792us/step - loss: 0.3244 - acc: 0.8563 - val_loss: 0.3449 - val_acc: 0.8498\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 14s 725us/step - loss: 0.3160 - acc: 0.8636 - val_loss: 0.3458 - val_acc: 0.8488\n",
      "19001/19001 [==============================] - 5s 263us/step\n",
      "999/999 [==============================] - 0s 242us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 16s 851us/step - loss: 0.4002 - acc: 0.8254 - val_loss: 0.3425 - val_acc: 0.8539\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 14s 724us/step - loss: 0.3454 - acc: 0.8508 - val_loss: 0.3361 - val_acc: 0.8559\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 15s 793us/step - loss: 0.3322 - acc: 0.8591 - val_loss: 0.3388 - val_acc: 0.8509\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 14s 729us/step - loss: 0.3220 - acc: 0.8624 - val_loss: 0.3391 - val_acc: 0.8549\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 15s 791us/step - loss: 0.3177 - acc: 0.8615 - val_loss: 0.3413 - val_acc: 0.8569\n",
      "19001/19001 [==============================] - 5s 257us/step\n",
      "999/999 [==============================] - 0s 271us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 15s 780us/step - loss: 0.3966 - acc: 0.8261 - val_loss: 0.3483 - val_acc: 0.8539\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 15s 779us/step - loss: 0.3431 - acc: 0.8503 - val_loss: 0.3307 - val_acc: 0.8549\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 14s 727us/step - loss: 0.3277 - acc: 0.8583 - val_loss: 0.3308 - val_acc: 0.8609\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 15s 788us/step - loss: 0.3214 - acc: 0.8622 - val_loss: 0.3171 - val_acc: 0.8659\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 14s 729us/step - loss: 0.3116 - acc: 0.8666 - val_loss: 0.3152 - val_acc: 0.8689\n",
      "19001/19001 [==============================] - 5s 268us/step\n",
      "999/999 [==============================] - 0s 259us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 19s 1ms/step - loss: 0.4019 - acc: 0.8227 - val_loss: 0.3544 - val_acc: 0.8398\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 14s 741us/step - loss: 0.3467 - acc: 0.8510 - val_loss: 0.3420 - val_acc: 0.8509\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 17s 910us/step - loss: 0.3319 - acc: 0.8564 - val_loss: 0.3406 - val_acc: 0.8509\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 15s 789us/step - loss: 0.3194 - acc: 0.8634 - val_loss: 0.3361 - val_acc: 0.8569\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 20s 1ms/step - loss: 0.3158 - acc: 0.8662 - val_loss: 0.3427 - val_acc: 0.8458\n",
      "19001/19001 [==============================] - 6s 330us/step\n",
      "999/999 [==============================] - 0s 279us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 17s 888us/step - loss: 0.3958 - acc: 0.8270 - val_loss: 0.3379 - val_acc: 0.8519\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 14s 741us/step - loss: 0.3462 - acc: 0.8495 - val_loss: 0.3246 - val_acc: 0.8639\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 14s 740us/step - loss: 0.3341 - acc: 0.8563 - val_loss: 0.3237 - val_acc: 0.8629\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 16s 865us/step - loss: 0.3192 - acc: 0.8641 - val_loss: 0.3284 - val_acc: 0.8669\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 15s 777us/step - loss: 0.3138 - acc: 0.8660 - val_loss: 0.3211 - val_acc: 0.8579\n",
      "19001/19001 [==============================] - 5s 264us/step\n",
      "999/999 [==============================] - 2s 2ms/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 15s 796us/step - loss: 0.4046 - acc: 0.8220 - val_loss: 0.3305 - val_acc: 0.8579\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 15s 797us/step - loss: 0.3498 - acc: 0.8470 - val_loss: 0.3160 - val_acc: 0.8649\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 14s 713us/step - loss: 0.3380 - acc: 0.8537 - val_loss: 0.3133 - val_acc: 0.8689\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 14s 722us/step - loss: 0.3309 - acc: 0.8567 - val_loss: 0.3069 - val_acc: 0.8609\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 17s 910us/step - loss: 0.3230 - acc: 0.8628 - val_loss: 0.3083 - val_acc: 0.8639\n",
      "19001/19001 [==============================] - 6s 320us/step\n",
      "999/999 [==============================] - 0s 373us/step\n",
      "Number of parameters =  202510\n",
      "Train on 19001 samples, validate on 999 samples\n",
      "Epoch 1/5\n",
      "19001/19001 [==============================] - 20s 1ms/step - loss: 0.4079 - acc: 0.8216 - val_loss: 0.3316 - val_acc: 0.8559\n",
      "Epoch 2/5\n",
      "19001/19001 [==============================] - 16s 858us/step - loss: 0.3494 - acc: 0.8467 - val_loss: 0.3331 - val_acc: 0.8589\n",
      "Epoch 3/5\n",
      "19001/19001 [==============================] - 16s 823us/step - loss: 0.3342 - acc: 0.8537 - val_loss: 0.3312 - val_acc: 0.8669\n",
      "Epoch 4/5\n",
      "19001/19001 [==============================] - 14s 748us/step - loss: 0.3217 - acc: 0.8602 - val_loss: 0.3305 - val_acc: 0.8649\n",
      "Epoch 5/5\n",
      "19001/19001 [==============================] - 15s 809us/step - loss: 0.3160 - acc: 0.8639 - val_loss: 0.3216 - val_acc: 0.8679\n",
      "19001/19001 [==============================] - 5s 269us/step\n",
      "999/999 [==============================] - 0s 255us/step\n",
      "0.856249351712\n"
     ]
    }
   ],
   "source": [
    "#Stratified K fold cross validation. We used stratified k-fold to have balanced training and test sets\n",
    "#This cell (which was modified after the fact) actually ended up producing the final model that we submitted\n",
    "\n",
    "sorted_train = np.array(train[np.array(sentiment_table.index)])\n",
    "#sorted_test = np.array(test[np.array(sentiment_table.index)])\n",
    "\n",
    "\n",
    "train_arr,y_arr = np.array(sorted_train),np.array(y)\n",
    "pred_list = []\n",
    "test_accs = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=20)\n",
    "s = 0 #running sum of test accuracy\n",
    "for train_index, test_index in skf.split(train_arr,y_arr):\n",
    "    X_train, X_test = train_arr[train_index],train_arr[test_index]\n",
    "    y_train,y_test = y_arr[train_index],y_arr[test_index]\n",
    "    model,train_acc,test_acc = train_cnn(X_train,y_train,X_test,y_test)\n",
    "    s+=test_acc\n",
    "    full_X_test = sorted_test.reshape(tuple(list(sorted_test.shape)+[1]))\n",
    "    preds = model.predict(full_X_test)\n",
    "    pred_list.append(preds)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    \n",
    "print s/20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters =  201318\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 13s 669us/step - loss: 0.4127 - acc: 0.8185\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 10s 517us/step - loss: 0.3461 - acc: 0.8501\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 10s 501us/step - loss: 0.3322 - acc: 0.8574\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 12s 583us/step - loss: 0.3183 - acc: 0.8619\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 10s 515us/step - loss: 0.3115 - acc: 0.8659\n"
     ]
    }
   ],
   "source": [
    "#Run final model on all training data - this one didn't do quite as well as the 20 fold bagged CNN\n",
    "\n",
    "# we must reshape the X data (add a channel dimension)\n",
    "full_X = sorted_train.reshape(tuple(list(sorted_train.shape)+[1]))\n",
    "y_arr = np.array(y)\n",
    "\n",
    "# we'll need to one-hot encode the labels\n",
    "full_y = keras.utils.np_utils.to_categorical(y_arr)\n",
    "\n",
    "ksize=2\n",
    "mpool=2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=ksize, padding='same',input_shape=(1000,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=mpool))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(8, kernel_size=ksize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=mpool))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print 'Number of parameters = ',model.count_params()\n",
    "\n",
    "model.compile(optimizer='adagrad',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(full_X, full_y, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get predictions for test data\n",
    "\n",
    "# we must reshape the X data (add a channel dimension)\n",
    "full_X_test = sorted_test.reshape(tuple(list(sorted_test.shape)+[1]))\n",
    "\n",
    "preds = model.predict(full_X_test)\n",
    "\n",
    "#Write file\n",
    "f = open('CNN_predictions_single1.csv','w')\n",
    "f.write('Id,Prediction\\n')\n",
    "for i, pred in enumerate(preds):\n",
    "    if pred[1] > 0.5:\n",
    "        f.write(str(i+1)+','+str(1)+'\\n')\n",
    "    else:\n",
    "        f.write(str(i+1)+','+str(0)+'\\n')\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bagging: Average the predictions from the 20 fold cross validated neural net from a few cells up\n",
    "\n",
    "preds = sum(pred_list)/20.\n",
    "\n",
    "\n",
    "#Write file\n",
    "f = open('CNN_predictions5.csv','w')\n",
    "f.write('Id,Prediction\\n')\n",
    "for i, pred in enumerate(preds):\n",
    "    if pred[1] > 0.5:\n",
    "        f.write(str(i+1)+','+str(1)+'\\n')\n",
    "    else:\n",
    "        f.write(str(i+1)+','+str(0)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Attempting to reweight the various neural nets in our bagging scheme\n",
    "#This didn't quite work out\n",
    "\n",
    "modified_weights = np.array(test_accs)-0.834\n",
    "preds2 = np.zeros(np.shape(pred_list[0]))\n",
    "for i, weight in enumerate(modified_weights):\n",
    "    preds2+= ((weight))*pred_list[i]\n",
    "preds = preds2/np.sum((modified_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.69379838,  11.64984434,   5.82829326,   2.29548232,\n",
       "         2.60817662,  10.32585357,  10.32585357,   0.81      ,\n",
       "         1.        ,   2.25      ,  12.25      ,   0.        ,\n",
       "         3.61      ,   2.20488314,   5.22435908,  12.1583802 ,\n",
       "         1.40324064,   5.6919739 ,   8.91850368,  11.47032474])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(modified_weights*100.)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [ 0.57412446  0.42587553]\n",
      "19 [ 0.59610183  0.40389816]\n",
      "25 [ 0.43860933  0.56139066]\n",
      "36 [ 0.52223366  0.47776634]\n",
      "40 [ 0.50553733  0.49446265]\n",
      "54 [ 0.41658175  0.58341824]\n",
      "58 [ 0.4407261   0.55927391]\n",
      "65 [ 0.47330973  0.52669025]\n",
      "81 [ 0.43407628  0.5659237 ]\n",
      "96 [ 0.43218184  0.56781815]\n",
      "114 [ 0.55719232  0.44280767]\n",
      "127 [ 0.44888212  0.55111788]\n",
      "130 [ 0.558782    0.44121798]\n",
      "153 [ 0.55510093  0.44489905]\n",
      "155 [ 0.56236746  0.43763253]\n",
      "162 [ 0.57451953  0.42548046]\n",
      "167 [ 0.56403531  0.4359647 ]\n",
      "170 [ 0.47987252  0.52012747]\n",
      "188 [ 0.41765185  0.58234814]\n",
      "191 [ 0.42207467  0.57792532]\n",
      "204 [ 0.48715752  0.51284246]\n",
      "221 [ 0.44753807  0.55246192]\n",
      "234 [ 0.48402859  0.51597141]\n",
      "251 [ 0.48164307  0.51835693]\n",
      "259 [ 0.53222236  0.46777764]\n",
      "266 [ 0.42636293  0.57363705]\n",
      "285 [ 0.497806  0.502194]\n",
      "292 [ 0.56217923  0.43782077]\n",
      "297 [ 0.53012633  0.46987368]\n",
      "298 [ 0.41388909  0.5861109 ]\n",
      "305 [ 0.50911507  0.49088494]\n",
      "309 [ 0.55772123  0.44227877]\n",
      "310 [ 0.51353357  0.48646643]\n",
      "320 [ 0.51868714  0.48131285]\n",
      "329 [ 0.47319956  0.52680044]\n",
      "350 [ 0.50627841  0.49372159]\n",
      "359 [ 0.40260406  0.59739594]\n",
      "374 [ 0.53601197  0.46398801]\n",
      "378 [ 0.59841742  0.40158257]\n",
      "396 [ 0.51341998  0.48658001]\n",
      "401 [ 0.44130323  0.55869678]\n",
      "430 [ 0.48082445  0.51917555]\n",
      "441 [ 0.5309717  0.4690283]\n",
      "445 [ 0.4928845   0.50711549]\n",
      "450 [ 0.41646814  0.58353184]\n",
      "455 [ 0.51040915  0.48959086]\n",
      "473 [ 0.57471524  0.42528476]\n",
      "479 [ 0.59125153  0.40874843]\n",
      "501 [ 0.43357819  0.5664218 ]\n",
      "525 [ 0.46224628  0.5377537 ]\n",
      "530 [ 0.49935197  0.50064801]\n",
      "540 [ 0.51891944  0.48108055]\n",
      "543 [ 0.44448197  0.55551803]\n",
      "566 [ 0.49501363  0.50498638]\n",
      "577 [ 0.4761617   0.52383829]\n",
      "580 [ 0.55436492  0.44563508]\n",
      "584 [ 0.4870421   0.51295789]\n",
      "585 [ 0.50100409  0.4989959 ]\n",
      "590 [ 0.55182051  0.44817949]\n",
      "592 [ 0.51752847  0.48247152]\n",
      "596 [ 0.57164101  0.42835899]\n",
      "597 [ 0.5497528   0.45024718]\n",
      "606 [ 0.56040361  0.43959639]\n",
      "607 [ 0.50110639  0.4988936 ]\n",
      "625 [ 0.56416927  0.43583072]\n",
      "634 [ 0.40244268  0.59755731]\n",
      "641 [ 0.43634137  0.56365859]\n",
      "647 [ 0.59936005  0.40063992]\n",
      "660 [ 0.47804155  0.52195843]\n",
      "666 [ 0.57449416  0.42550583]\n",
      "671 [ 0.4119951  0.5880049]\n",
      "676 [ 0.48795724  0.51204274]\n",
      "691 [ 0.49447794  0.50552206]\n",
      "696 [ 0.44796527  0.55203475]\n",
      "704 [ 0.52365756  0.47634241]\n",
      "709 [ 0.5863318   0.41366819]\n",
      "716 [ 0.40592072  0.59407928]\n",
      "729 [ 0.56512627  0.43487374]\n",
      "732 [ 0.48217965  0.51782036]\n",
      "744 [ 0.53326946  0.46673053]\n",
      "746 [ 0.49194593  0.50805405]\n",
      "759 [ 0.5199446   0.48005538]\n",
      "760 [ 0.41143498  0.588565  ]\n",
      "768 [ 0.54156084  0.45843915]\n",
      "773 [ 0.59282603  0.40717395]\n",
      "776 [ 0.58575032  0.41424965]\n",
      "783 [ 0.40132889  0.59867111]\n",
      "785 [ 0.52625211  0.4737479 ]\n",
      "787 [ 0.51341534  0.48658465]\n",
      "793 [ 0.53929566  0.46070434]\n",
      "795 [ 0.46404802  0.53595198]\n",
      "796 [ 0.44303252  0.55696748]\n",
      "804 [ 0.53733524  0.46266473]\n",
      "812 [ 0.46611392  0.53388608]\n",
      "830 [ 0.41967171  0.58032829]\n",
      "846 [ 0.56724313  0.43275687]\n",
      "863 [ 0.49172988  0.50827011]\n",
      "873 [ 0.4468336  0.5531664]\n",
      "881 [ 0.49773567  0.50226435]\n",
      "882 [ 0.53849294  0.46150704]\n",
      "892 [ 0.44919144  0.55080854]\n",
      "895 [ 0.47204728  0.52795271]\n",
      "909 [ 0.4786371   0.52136289]\n",
      "913 [ 0.56625239  0.43374758]\n",
      "930 [ 0.56372828  0.43627171]\n",
      "939 [ 0.50312512  0.49687487]\n",
      "941 [ 0.42349441  0.57650561]\n",
      "942 [ 0.43726476  0.56273523]\n",
      "949 [ 0.57790449  0.42209551]\n",
      "954 [ 0.57490243  0.42509756]\n",
      "963 [ 0.59674203  0.40325795]\n",
      "970 [ 0.45162339  0.54837658]\n",
      "974 [ 0.55181842  0.44818156]\n",
      "981 [ 0.52209996  0.47790003]\n",
      "991 [ 0.42693317  0.57306683]\n",
      "992 [ 0.55147803  0.44852197]\n",
      "1000 [ 0.55942964  0.44057038]\n",
      "1060 [ 0.51867454  0.48132544]\n",
      "1064 [ 0.49328015  0.50671986]\n",
      "1087 [ 0.56677718  0.43322283]\n",
      "1141 [ 0.48470338  0.51529662]\n",
      "1169 [ 0.58893746  0.41106255]\n",
      "1193 [ 0.47113319  0.52886682]\n",
      "1227 [ 0.47882311  0.52117691]\n",
      "1237 [ 0.40780166  0.59219833]\n",
      "1245 [ 0.45571676  0.54428323]\n",
      "1261 [ 0.53730427  0.46269572]\n",
      "1312 [ 0.45523176  0.54476824]\n",
      "1322 [ 0.44230085  0.55769913]\n",
      "1329 [ 0.46259639  0.53740359]\n",
      "1339 [ 0.4018105  0.5981895]\n",
      "1358 [ 0.5485359   0.45146411]\n",
      "1360 [ 0.48415902  0.51584098]\n",
      "1368 [ 0.44263849  0.5573615 ]\n",
      "1379 [ 0.47197753  0.52802246]\n",
      "1381 [ 0.41602053  0.58397945]\n",
      "1385 [ 0.58812919  0.4118708 ]\n",
      "1387 [ 0.4643957   0.53560428]\n",
      "1404 [ 0.59979776  0.40020224]\n",
      "1408 [ 0.57174702  0.42825297]\n",
      "1409 [ 0.40027527  0.59972474]\n",
      "1442 [ 0.50522074  0.49477926]\n",
      "1443 [ 0.42006093  0.57993905]\n",
      "1444 [ 0.43916625  0.56083376]\n",
      "1466 [ 0.40753269  0.59246731]\n",
      "1488 [ 0.534196    0.46580399]\n",
      "1503 [ 0.5082053   0.49179471]\n",
      "1512 [ 0.58582148  0.41417851]\n",
      "1528 [ 0.51207875  0.48792126]\n",
      "1530 [ 0.43656056  0.56343942]\n",
      "1533 [ 0.48662547  0.51337454]\n",
      "1534 [ 0.40853928  0.59146069]\n",
      "1540 [ 0.54902249  0.4509775 ]\n",
      "1541 [ 0.52083912  0.47916084]\n",
      "1544 [ 0.46968101  0.53031898]\n",
      "1547 [ 0.56035154  0.43964845]\n",
      "1578 [ 0.57205391  0.42794608]\n",
      "1584 [ 0.44627116  0.55372883]\n",
      "1588 [ 0.59943214  0.40056783]\n",
      "1590 [ 0.53539736  0.46460263]\n",
      "1602 [ 0.43057478  0.56942523]\n",
      "1605 [ 0.56950043  0.43049955]\n",
      "1623 [ 0.5717712  0.4282288]\n",
      "1631 [ 0.46064183  0.53935816]\n",
      "1635 [ 0.43690071  0.56309929]\n",
      "1639 [ 0.41337953  0.58662049]\n",
      "1657 [ 0.59811259  0.4018874 ]\n",
      "1682 [ 0.55893559  0.44106441]\n",
      "1696 [ 0.47644248  0.52355752]\n",
      "1698 [ 0.48902597  0.51097403]\n",
      "1704 [ 0.42142687  0.57857312]\n",
      "1710 [ 0.59491648  0.40508352]\n",
      "1715 [ 0.40785235  0.59214764]\n",
      "1717 [ 0.42906162  0.57093835]\n",
      "1722 [ 0.48282221  0.51717779]\n",
      "1725 [ 0.51292857  0.48707144]\n",
      "1755 [ 0.51435494  0.48564506]\n",
      "1760 [ 0.53051147  0.46948854]\n",
      "1768 [ 0.42143327  0.5785667 ]\n",
      "1789 [ 0.50629738  0.49370261]\n",
      "1796 [ 0.44810875  0.55189125]\n",
      "1802 [ 0.46596878  0.53403121]\n",
      "1843 [ 0.47348869  0.52651133]\n",
      "1854 [ 0.44390815  0.55609187]\n",
      "1864 [ 0.43155737  0.56844262]\n",
      "1866 [ 0.49009954  0.50990046]\n",
      "1869 [ 0.42761897  0.572381  ]\n",
      "1870 [ 0.48053489  0.51946508]\n",
      "1873 [ 0.43738351  0.56261647]\n",
      "1917 [ 0.55179309  0.44820692]\n",
      "1921 [ 0.51989322  0.48010679]\n",
      "1937 [ 0.42508644  0.57491356]\n",
      "1962 [ 0.58203087  0.41796913]\n",
      "1970 [ 0.45912363  0.54087639]\n",
      "1991 [ 0.49202935  0.50797063]\n",
      "2033 [ 0.52056617  0.47943382]\n",
      "2042 [ 0.41513851  0.58486148]\n",
      "2060 [ 0.4266512  0.5733488]\n",
      "2068 [ 0.44793349  0.55206653]\n",
      "2082 [ 0.5413389   0.45866112]\n",
      "2086 [ 0.45893629  0.54106373]\n",
      "2112 [ 0.41043652  0.58956346]\n",
      "2118 [ 0.47123864  0.52876137]\n",
      "2119 [ 0.49342274  0.50657724]\n",
      "2170 [ 0.5192189  0.4807811]\n",
      "2187 [ 0.55068934  0.44931066]\n",
      "2188 [ 0.4468851   0.55311488]\n",
      "2194 [ 0.51902612  0.48097389]\n",
      "2211 [ 0.55532215  0.44467785]\n",
      "2247 [ 0.51328142  0.48671858]\n",
      "2249 [ 0.45134741  0.54865257]\n",
      "2262 [ 0.53521813  0.46478187]\n",
      "2267 [ 0.40994589  0.5900541 ]\n",
      "2269 [ 0.57259469  0.42740531]\n",
      "2279 [ 0.41160224  0.58839775]\n",
      "2286 [ 0.44258312  0.55741687]\n",
      "2316 [ 0.48813172  0.51186828]\n",
      "2329 [ 0.50975188  0.49024813]\n",
      "2333 [ 0.43908201  0.560918  ]\n",
      "2343 [ 0.59201483  0.40798516]\n",
      "2368 [ 0.46180064  0.53819934]\n",
      "2396 [ 0.50922661  0.4907734 ]\n",
      "2398 [ 0.55386847  0.44613154]\n",
      "2412 [ 0.43964876  0.56035121]\n",
      "2432 [ 0.44230036  0.55769963]\n",
      "2448 [ 0.42386421  0.57613579]\n",
      "2460 [ 0.50187079  0.49812918]\n",
      "2471 [ 0.44539812  0.55460187]\n",
      "2472 [ 0.44619364  0.55380634]\n",
      "2473 [ 0.42439821  0.57560179]\n",
      "2495 [ 0.54054012  0.45945987]\n",
      "2514 [ 0.59014633  0.40985367]\n",
      "2519 [ 0.52116481  0.4788352 ]\n",
      "2527 [ 0.58282686  0.41717313]\n",
      "2546 [ 0.52365559  0.4763444 ]\n",
      "2560 [ 0.41396357  0.58603642]\n",
      "2563 [ 0.4571044  0.5428956]\n",
      "2589 [ 0.55548767  0.44451232]\n",
      "2593 [ 0.5189914   0.48100859]\n",
      "2600 [ 0.51589408  0.48410591]\n",
      "2608 [ 0.55846983  0.44153015]\n",
      "2616 [ 0.46272779  0.5372722 ]\n",
      "2638 [ 0.57819174  0.42180824]\n",
      "2650 [ 0.54673213  0.45326787]\n",
      "2653 [ 0.58209536  0.41790462]\n",
      "2661 [ 0.57917106  0.4208289 ]\n",
      "2673 [ 0.56717416  0.43282584]\n",
      "2688 [ 0.50684714  0.49315285]\n",
      "2695 [ 0.45777036  0.54222961]\n",
      "2710 [ 0.40504374  0.59495627]\n",
      "2718 [ 0.40169673  0.59830325]\n",
      "2720 [ 0.47685364  0.52314634]\n",
      "2758 [ 0.40629198  0.59370802]\n",
      "2764 [ 0.4028718   0.59712819]\n",
      "2773 [ 0.4968089   0.50319108]\n",
      "2780 [ 0.56931826  0.43068174]\n",
      "2787 [ 0.54029146  0.45970854]\n",
      "2809 [ 0.58211079  0.41788924]\n",
      "2814 [ 0.55528785  0.44471214]\n",
      "2823 [ 0.53973104  0.46026896]\n",
      "2842 [ 0.5460526   0.45394741]\n",
      "2879 [ 0.5832319   0.41676809]\n",
      "2884 [ 0.41249732  0.58750267]\n",
      "2890 [ 0.59221133  0.40778867]\n",
      "2892 [ 0.48585573  0.51414427]\n",
      "2898 [ 0.41379476  0.58620525]\n",
      "2906 [ 0.58367425  0.41632572]\n",
      "2925 [ 0.58170748  0.41829252]\n",
      "2930 [ 0.45083649  0.54916351]\n",
      "2934 [ 0.49814602  0.50185395]\n",
      "2941 [ 0.40854111  0.59145888]\n",
      "2945 [ 0.50766508  0.49233492]\n",
      "2952 [ 0.58317096  0.41682901]\n",
      "2960 [ 0.41727493  0.58272506]\n",
      "2963 [ 0.45455922  0.54544077]\n",
      "2967 [ 0.57260608  0.42739393]\n",
      "2980 [ 0.45779434  0.54220566]\n",
      "2981 [ 0.41949098  0.580509  ]\n",
      "3000 [ 0.50384607  0.49615391]\n",
      "3011 [ 0.46242331  0.53757669]\n",
      "3013 [ 0.40610511  0.59389487]\n",
      "3026 [ 0.46631002  0.53368998]\n",
      "3035 [ 0.54659124  0.45340877]\n",
      "3038 [ 0.47183012  0.52816988]\n",
      "3042 [ 0.45356308  0.54643689]\n",
      "3051 [ 0.45110359  0.54889638]\n",
      "3058 [ 0.53378811  0.46621189]\n",
      "3087 [ 0.5801648   0.41983519]\n",
      "3091 [ 0.5673703   0.43262969]\n",
      "3099 [ 0.55139316  0.44860684]\n",
      "3102 [ 0.41758068  0.58241932]\n",
      "3119 [ 0.53940674  0.46059327]\n",
      "3123 [ 0.49088424  0.50911575]\n",
      "3135 [ 0.49925395  0.50074604]\n",
      "3136 [ 0.56822979  0.4317702 ]\n",
      "3166 [ 0.47886511  0.52113491]\n",
      "3168 [ 0.46221616  0.53778384]\n",
      "3169 [ 0.45209266  0.54790733]\n",
      "3171 [ 0.41383957  0.58616042]\n",
      "3176 [ 0.57330624  0.42669376]\n",
      "3178 [ 0.45681183  0.54318816]\n",
      "3183 [ 0.5795675   0.42043246]\n",
      "3194 [ 0.48048131  0.51951865]\n",
      "3201 [ 0.40897904  0.59102094]\n",
      "3204 [ 0.40817046  0.59182953]\n",
      "3238 [ 0.57877247  0.4212275 ]\n",
      "3248 [ 0.42138592  0.57861406]\n",
      "3250 [ 0.47148966  0.52851033]\n",
      "3264 [ 0.40600389  0.59399609]\n",
      "3299 [ 0.47095424  0.52904575]\n",
      "3320 [ 0.55656249  0.4434375 ]\n",
      "3322 [ 0.57002388  0.42997611]\n",
      "3324 [ 0.50924733  0.49075267]\n",
      "3325 [ 0.5715474   0.42845259]\n",
      "3364 [ 0.46065037  0.53934962]\n",
      "3381 [ 0.41728722  0.58271276]\n",
      "3383 [ 0.57393925  0.42606075]\n",
      "3387 [ 0.4264587   0.57354128]\n",
      "3393 [ 0.4166853   0.58331469]\n",
      "3401 [ 0.47806595  0.52193405]\n",
      "3408 [ 0.41599923  0.58400077]\n",
      "3415 [ 0.4509882  0.5490118]\n",
      "3417 [ 0.41168852  0.58831145]\n",
      "3434 [ 0.47116838  0.52883163]\n",
      "3456 [ 0.41352278  0.58647722]\n",
      "3466 [ 0.58461918  0.41538083]\n",
      "3467 [ 0.4737497  0.5262503]\n",
      "3468 [ 0.58110821  0.41889177]\n",
      "3476 [ 0.48571686  0.51428314]\n",
      "3477 [ 0.43194219  0.56805781]\n",
      "3517 [ 0.50620458  0.4937954 ]\n",
      "3546 [ 0.50495676  0.49504325]\n",
      "3553 [ 0.54994643  0.45005357]\n",
      "3578 [ 0.431475  0.568525]\n",
      "3591 [ 0.49788679  0.5021132 ]\n",
      "3644 [ 0.43045016  0.56954983]\n",
      "3649 [ 0.52129127  0.47870872]\n",
      "3696 [ 0.49230942  0.50769057]\n",
      "3698 [ 0.4012405   0.59875951]\n",
      "3709 [ 0.42525093  0.57474905]\n",
      "3723 [ 0.51559931  0.48440068]\n",
      "3728 [ 0.41794726  0.58205276]\n",
      "3733 [ 0.5785798   0.42142019]\n",
      "3742 [ 0.51194561  0.48805439]\n",
      "3750 [ 0.40494052  0.59505948]\n",
      "3758 [ 0.41072439  0.58927558]\n",
      "3760 [ 0.54104681  0.45895318]\n",
      "3769 [ 0.45664911  0.54335088]\n",
      "3797 [ 0.40637204  0.59362795]\n",
      "3811 [ 0.43481176  0.56518825]\n",
      "3813 [ 0.50738517  0.49261482]\n",
      "3846 [ 0.52418612  0.47581387]\n",
      "3847 [ 0.5583799   0.44162011]\n",
      "3870 [ 0.54762156  0.45237844]\n",
      "3881 [ 0.5406655   0.45933448]\n",
      "3900 [ 0.5988141   0.40118589]\n",
      "3921 [ 0.51856183  0.48143815]\n",
      "3924 [ 0.44204868  0.55795132]\n",
      "3932 [ 0.44003031  0.55996971]\n",
      "3937 [ 0.40185368  0.5981463 ]\n",
      "3941 [ 0.50675156  0.49324843]\n",
      "3956 [ 0.52780506  0.47219492]\n",
      "3957 [ 0.59483456  0.40516541]\n",
      "3976 [ 0.48236946  0.51763054]\n",
      "3982 [ 0.49004593  0.50995406]\n",
      "3991 [ 0.51911456  0.48088546]\n",
      "3999 [ 0.5990184   0.40098158]\n",
      "4024 [ 0.44657264  0.55342736]\n",
      "4028 [ 0.49207554  0.50792445]\n",
      "4033 [ 0.55104479  0.44895521]\n",
      "4046 [ 0.54508388  0.45491611]\n",
      "4058 [ 0.50871818  0.49128182]\n",
      "4071 [ 0.54190851  0.45809149]\n",
      "4078 [ 0.41523309  0.58476689]\n",
      "4083 [ 0.50221369  0.49778629]\n",
      "4108 [ 0.50800997  0.49199002]\n",
      "4148 [ 0.54145315  0.45854685]\n",
      "4156 [ 0.40742601  0.59257397]\n",
      "4162 [ 0.40727084  0.59272916]\n",
      "4175 [ 0.42869385  0.57130612]\n",
      "4179 [ 0.53804417  0.46195582]\n",
      "4181 [ 0.4904606  0.5095394]\n",
      "4197 [ 0.54628838  0.45371161]\n",
      "4203 [ 0.53117466  0.46882533]\n",
      "4204 [ 0.54303087  0.45696913]\n",
      "4214 [ 0.51360758  0.48639242]\n",
      "4215 [ 0.51979553  0.48020448]\n",
      "4223 [ 0.55257623  0.44742376]\n",
      "4230 [ 0.58118917  0.41881082]\n",
      "4244 [ 0.50035998  0.49964   ]\n",
      "4249 [ 0.53628055  0.46371944]\n",
      "4259 [ 0.53406462  0.46593536]\n",
      "4265 [ 0.41559733  0.58440265]\n",
      "4269 [ 0.54503737  0.45496261]\n",
      "4276 [ 0.47195483  0.52804515]\n",
      "4288 [ 0.58149912  0.41850086]\n",
      "4301 [ 0.51465201  0.485348  ]\n",
      "4321 [ 0.44461342  0.5553866 ]\n",
      "4323 [ 0.40802492  0.59197508]\n",
      "4331 [ 0.44947071  0.55052928]\n",
      "4339 [ 0.57747041  0.42252959]\n",
      "4354 [ 0.44652715  0.55347283]\n",
      "4356 [ 0.48562286  0.51437713]\n",
      "4359 [ 0.57071977  0.42928022]\n",
      "4372 [ 0.40722157  0.59277842]\n",
      "4378 [ 0.58285528  0.41714471]\n",
      "4379 [ 0.48485343  0.51514658]\n",
      "4384 [ 0.46916627  0.53083372]\n",
      "4402 [ 0.44587988  0.55412012]\n",
      "4413 [ 0.40565953  0.59434048]\n",
      "4416 [ 0.50260313  0.49739688]\n",
      "4417 [ 0.5770419   0.42295809]\n",
      "4443 [ 0.56304362  0.43695637]\n",
      "4473 [ 0.47722503  0.52277498]\n",
      "4476 [ 0.41939546  0.58060454]\n",
      "4503 [ 0.49923028  0.50076972]\n",
      "4510 [ 0.59640112  0.40359887]\n",
      "4520 [ 0.41146911  0.58853088]\n",
      "4534 [ 0.59665897  0.40334104]\n",
      "4543 [ 0.55802791  0.44197209]\n",
      "4551 [ 0.40986823  0.59013177]\n",
      "4553 [ 0.59085985  0.40914014]\n",
      "4556 [ 0.500782    0.49921798]\n",
      "4559 [ 0.53921493  0.46078506]\n",
      "4565 [ 0.54488388  0.45511612]\n",
      "4575 [ 0.43487833  0.56512168]\n",
      "4576 [ 0.54349385  0.45650612]\n",
      "4578 [ 0.52986486  0.47013514]\n",
      "4586 [ 0.40823453  0.59176548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4589 [ 0.56132068  0.4386793 ]\n",
      "4630 [ 0.59551937  0.40448063]\n",
      "4631 [ 0.5455512   0.45444881]\n",
      "4680 [ 0.54270885  0.45729114]\n",
      "4710 [ 0.44357176  0.55642822]\n",
      "4727 [ 0.42828218  0.57171781]\n",
      "4738 [ 0.47167851  0.52832149]\n",
      "4742 [ 0.46012993  0.53987005]\n",
      "4756 [ 0.41713332  0.58286666]\n",
      "4763 [ 0.57965378  0.42034619]\n",
      "4766 [ 0.54282328  0.45717671]\n",
      "4767 [ 0.55904889  0.44095112]\n",
      "4778 [ 0.53739301  0.46260699]\n",
      "4787 [ 0.57339093  0.42660906]\n",
      "4788 [ 0.51045836  0.48954163]\n",
      "4798 [ 0.51634428  0.48365571]\n",
      "4803 [ 0.59793045  0.40206957]\n",
      "4812 [ 0.50299094  0.49700905]\n",
      "4813 [ 0.53595644  0.46404354]\n",
      "4831 [ 0.54934188  0.45065813]\n",
      "4841 [ 0.41231565  0.58768434]\n",
      "4850 [ 0.4884595   0.51154053]\n",
      "4875 [ 0.44470425  0.55529573]\n",
      "4889 [ 0.53297139  0.4670286 ]\n",
      "4899 [ 0.44849011  0.55150988]\n",
      "4900 [ 0.42865956  0.57134043]\n",
      "4901 [ 0.46368158  0.53631839]\n",
      "4918 [ 0.55969897  0.44030103]\n",
      "4930 [ 0.53546673  0.46453327]\n",
      "4932 [ 0.46874085  0.53125913]\n",
      "4934 [ 0.48206789  0.51793209]\n",
      "4953 [ 0.46259917  0.5374008 ]\n",
      "4961 [ 0.45440353  0.54559644]\n",
      "4970 [ 0.44046653  0.55953347]\n",
      "4977 [ 0.45284014  0.54715985]\n",
      "4994 [ 0.4433705   0.55662948]\n",
      "5011 [ 0.51269627  0.48730371]\n",
      "5016 [ 0.43080373  0.56919627]\n",
      "5040 [ 0.58355007  0.4164499 ]\n",
      "5049 [ 0.59902576  0.40097424]\n",
      "5052 [ 0.44837274  0.55162726]\n",
      "5061 [ 0.56501473  0.43498525]\n",
      "5090 [ 0.58194464  0.41805535]\n",
      "5092 [ 0.52843246  0.47156754]\n",
      "5093 [ 0.42843114  0.57156886]\n",
      "5095 [ 0.41170203  0.58829796]\n",
      "5098 [ 0.41739977  0.58260024]\n",
      "5136 [ 0.41978477  0.58021522]\n",
      "5147 [ 0.59094194  0.40905805]\n",
      "5151 [ 0.45481118  0.54518879]\n",
      "5165 [ 0.48755872  0.51244128]\n",
      "5193 [ 0.48032467  0.51967533]\n",
      "5201 [ 0.5361958   0.46380419]\n",
      "5233 [ 0.42420348  0.57579651]\n",
      "5242 [ 0.59131145  0.40868854]\n",
      "5248 [ 0.53995633  0.46004366]\n",
      "5256 [ 0.41145036  0.58854964]\n",
      "5262 [ 0.47037323  0.52962679]\n",
      "5264 [ 0.58531021  0.41468979]\n",
      "5270 [ 0.54913351  0.45086648]\n",
      "5272 [ 0.57941234  0.42058767]\n",
      "5283 [ 0.47201173  0.52798827]\n",
      "5315 [ 0.58784452  0.41215547]\n",
      "5316 [ 0.52743989  0.47256011]\n",
      "5354 [ 0.41353053  0.58646945]\n",
      "5371 [ 0.57290809  0.42709189]\n",
      "5393 [ 0.4137002   0.58629979]\n",
      "5396 [ 0.53690697  0.463093  ]\n",
      "5410 [ 0.5543981   0.44560189]\n",
      "5431 [ 0.45684876  0.54315122]\n",
      "5467 [ 0.49438428  0.5056157 ]\n",
      "5498 [ 0.44062487  0.55937513]\n",
      "5515 [ 0.4576656   0.54233439]\n",
      "5550 [ 0.52136918  0.47863084]\n",
      "5551 [ 0.49551569  0.5044843 ]\n",
      "5556 [ 0.51717147  0.48282852]\n",
      "5568 [ 0.50946771  0.49053228]\n",
      "5569 [ 0.47439693  0.52560307]\n",
      "5592 [ 0.53414133  0.46585866]\n",
      "5602 [ 0.48557219  0.51442778]\n",
      "5614 [ 0.55141159  0.44858841]\n",
      "5638 [ 0.43487734  0.56512266]\n",
      "5643 [ 0.54484436  0.45515565]\n",
      "5658 [ 0.59734121  0.40265879]\n",
      "5675 [ 0.57968769  0.42031229]\n",
      "5679 [ 0.54590301  0.45409699]\n",
      "5685 [ 0.48275523  0.51724476]\n",
      "5699 [ 0.4080714   0.59192859]\n",
      "5707 [ 0.54811713  0.45188287]\n",
      "5735 [ 0.50738953  0.49261045]\n",
      "5738 [ 0.52594966  0.47405034]\n",
      "5742 [ 0.57314208  0.42685793]\n",
      "5764 [ 0.41006049  0.58993949]\n",
      "5775 [ 0.48277985  0.51722015]\n",
      "5781 [ 0.41804709  0.58195289]\n",
      "5783 [ 0.48871966  0.51128032]\n",
      "5790 [ 0.4109099   0.58909009]\n",
      "5834 [ 0.59648278  0.4035172 ]\n",
      "5861 [ 0.43787346  0.56212654]\n",
      "5868 [ 0.59960047  0.40039952]\n",
      "5882 [ 0.55210234  0.44789766]\n",
      "5918 [ 0.45971773  0.54028227]\n",
      "5945 [ 0.47075624  0.52924376]\n",
      "5950 [ 0.48892105  0.51107896]\n",
      "5957 [ 0.54321258  0.4567874 ]\n",
      "5958 [ 0.56757947  0.43242052]\n",
      "5962 [ 0.54683462  0.45316537]\n",
      "5971 [ 0.56883311  0.43116688]\n",
      "5981 [ 0.49776509  0.50223491]\n",
      "5991 [ 0.44062032  0.55937967]\n",
      "5993 [ 0.43415989  0.56584011]\n",
      "5997 [ 0.57385415  0.42614583]\n",
      "6006 [ 0.4930853  0.5069147]\n",
      "6033 [ 0.57000897  0.42999103]\n",
      "6058 [ 0.44216228  0.5578377 ]\n",
      "6068 [ 0.51136425  0.48863575]\n",
      "6070 [ 0.46765427  0.53234572]\n",
      "6083 [ 0.49422951  0.50577045]\n",
      "6087 [ 0.59706079  0.40293921]\n",
      "6109 [ 0.44279365  0.55720635]\n",
      "6133 [ 0.51749402  0.48250598]\n",
      "6143 [ 0.56992028  0.43007972]\n",
      "6146 [ 0.49213191  0.50786807]\n",
      "6151 [ 0.48992431  0.51007568]\n",
      "6159 [ 0.57588379  0.42411619]\n",
      "6173 [ 0.52250959  0.47749041]\n",
      "6199 [ 0.53830106  0.46169892]\n",
      "6227 [ 0.5604526   0.43954739]\n",
      "6231 [ 0.4342535   0.56574648]\n",
      "6248 [ 0.55572897  0.44427102]\n",
      "6267 [ 0.40743587  0.59256413]\n",
      "6272 [ 0.41573778  0.58426222]\n",
      "6278 [ 0.58011952  0.41988044]\n",
      "6290 [ 0.4775462   0.52245379]\n",
      "6294 [ 0.59614676  0.40385325]\n",
      "6322 [ 0.52817657  0.47182342]\n",
      "6327 [ 0.47918376  0.52081622]\n",
      "6342 [ 0.52722248  0.47277752]\n",
      "6343 [ 0.53607659  0.46392339]\n",
      "6345 [ 0.5842243   0.41577567]\n",
      "6346 [ 0.56759787  0.43240213]\n",
      "6353 [ 0.44817175  0.55182824]\n",
      "6370 [ 0.4798064  0.5201936]\n",
      "6372 [ 0.53316735  0.46683265]\n",
      "6374 [ 0.45475939  0.54524061]\n",
      "6375 [ 0.51232204  0.48767795]\n",
      "6378 [ 0.5121175   0.48788251]\n",
      "6393 [ 0.5494314   0.45056859]\n",
      "6401 [ 0.53185488  0.46814512]\n",
      "6415 [ 0.45921972  0.54078026]\n",
      "6417 [ 0.48500333  0.51499665]\n",
      "6419 [ 0.43123382  0.56876618]\n",
      "6440 [ 0.51433845  0.48566154]\n",
      "6448 [ 0.41266979  0.5873302 ]\n",
      "6480 [ 0.46026885  0.53973113]\n",
      "6485 [ 0.45870323  0.54129676]\n",
      "6488 [ 0.4504102   0.54958979]\n",
      "6505 [ 0.45407497  0.54592503]\n",
      "6511 [ 0.47477689  0.52522311]\n",
      "6513 [ 0.5239249   0.47607509]\n",
      "6519 [ 0.51500172  0.48499828]\n",
      "6521 [ 0.59150265  0.40849735]\n",
      "6531 [ 0.43171125  0.56828875]\n",
      "6533 [ 0.433991  0.566009]\n",
      "6536 [ 0.58226329  0.41773671]\n",
      "6540 [ 0.45840666  0.54159333]\n",
      "6541 [ 0.55815972  0.44184026]\n",
      "6545 [ 0.5575373   0.44246271]\n",
      "6587 [ 0.47392885  0.52607113]\n",
      "6594 [ 0.48821053  0.51178945]\n",
      "6600 [ 0.5521674   0.44783257]\n",
      "6601 [ 0.48297912  0.51702089]\n",
      "6602 [ 0.51090985  0.48909014]\n",
      "6619 [ 0.46667666  0.53332334]\n",
      "6628 [ 0.58403399  0.41596599]\n",
      "6636 [ 0.57383533  0.42616467]\n",
      "6644 [ 0.42973794  0.57026206]\n",
      "6675 [ 0.55701265  0.44298734]\n",
      "6701 [ 0.59380005  0.40619993]\n",
      "6702 [ 0.56867004  0.43132995]\n",
      "6715 [ 0.51708976  0.48291024]\n",
      "6719 [ 0.47886884  0.52113115]\n",
      "6720 [ 0.42442093  0.57557907]\n",
      "6732 [ 0.4172577   0.58274228]\n",
      "6739 [ 0.48093401  0.51906599]\n",
      "6753 [ 0.51460075  0.48539924]\n",
      "6755 [ 0.59477095  0.40522905]\n",
      "6778 [ 0.44324591  0.55675407]\n",
      "6779 [ 0.52849885  0.47150114]\n",
      "6790 [ 0.5436117  0.4563883]\n",
      "6799 [ 0.48019993  0.51980006]\n",
      "6800 [ 0.46481983  0.53518018]\n",
      "6802 [ 0.52529903  0.47470097]\n",
      "6807 [ 0.56458193  0.43541806]\n",
      "6828 [ 0.5123805   0.48761949]\n",
      "6846 [ 0.49059339  0.5094066 ]\n",
      "6849 [ 0.43382579  0.56617421]\n",
      "6854 [ 0.52446388  0.4755361 ]\n",
      "6864 [ 0.51354484  0.48645515]\n",
      "6868 [ 0.55343944  0.44656055]\n",
      "6894 [ 0.45795794  0.54204206]\n",
      "6902 [ 0.40462508  0.59537492]\n",
      "6903 [ 0.56894536  0.43105463]\n",
      "6908 [ 0.50318355  0.49681645]\n",
      "6924 [ 0.48693061  0.51306939]\n",
      "6942 [ 0.44444162  0.55555838]\n",
      "6943 [ 0.48956748  0.51043251]\n",
      "6955 [ 0.59580864  0.40419136]\n",
      "6964 [ 0.52370314  0.47629685]\n",
      "6966 [ 0.54855968  0.45144032]\n",
      "6973 [ 0.5357487  0.4642513]\n",
      "6980 [ 0.55288723  0.44711275]\n",
      "6983 [ 0.41100633  0.58899366]\n",
      "7052 [ 0.50614214  0.49385788]\n",
      "7066 [ 0.47746279  0.52253719]\n",
      "7067 [ 0.43840297  0.56159699]\n",
      "7106 [ 0.58825484  0.41174516]\n",
      "7121 [ 0.58330644  0.41669354]\n",
      "7122 [ 0.51703895  0.48296102]\n",
      "7125 [ 0.46941774  0.53058226]\n",
      "7127 [ 0.56993504  0.43006497]\n",
      "7172 [ 0.45649131  0.5435087 ]\n",
      "7175 [ 0.49442271  0.50557726]\n",
      "7192 [ 0.42979626  0.57020374]\n",
      "7195 [ 0.50150131  0.49849869]\n",
      "7196 [ 0.52223153  0.47776846]\n",
      "7198 [ 0.53323469  0.46676531]\n",
      "7223 [ 0.5451149   0.45488508]\n",
      "7225 [ 0.51911577  0.48088423]\n",
      "7242 [ 0.55943764  0.44056237]\n",
      "7282 [ 0.53517467  0.46482532]\n",
      "7289 [ 0.5355336   0.46446638]\n",
      "7304 [ 0.4136884   0.58631162]\n",
      "7308 [ 0.43308223  0.56691773]\n",
      "7327 [ 0.42086331  0.57913668]\n",
      "7345 [ 0.49586154  0.50413845]\n",
      "7350 [ 0.49856869  0.50143129]\n",
      "7351 [ 0.47754708  0.52245293]\n",
      "7353 [ 0.42706891  0.57293109]\n",
      "7362 [ 0.53053487  0.46946512]\n",
      "7407 [ 0.5177982  0.4822018]\n",
      "7413 [ 0.41796195  0.58203806]\n",
      "7414 [ 0.47891697  0.52108301]\n",
      "7424 [ 0.58327108  0.41672889]\n",
      "7427 [ 0.4730529   0.52694709]\n",
      "7429 [ 0.42384078  0.57615922]\n",
      "7430 [ 0.47341731  0.52658267]\n",
      "7435 [ 0.46294508  0.53705491]\n",
      "7457 [ 0.43630254  0.56369744]\n",
      "7463 [ 0.47938211  0.52061789]\n",
      "7476 [ 0.54811043  0.45188957]\n",
      "7489 [ 0.53932818  0.4606718 ]\n",
      "7494 [ 0.4728791  0.5271209]\n",
      "7506 [ 0.47560591  0.5243941 ]\n",
      "7507 [ 0.54335214  0.45664783]\n",
      "7508 [ 0.5012967   0.49870331]\n",
      "7509 [ 0.53034537  0.46965461]\n",
      "7519 [ 0.42216308  0.57783694]\n",
      "7552 [ 0.41445655  0.58554344]\n",
      "7590 [ 0.4940671   0.50593288]\n",
      "7591 [ 0.49793334  0.50206666]\n",
      "7595 [ 0.53906267  0.46093732]\n",
      "7610 [ 0.58933591  0.41066407]\n",
      "7632 [ 0.51547713  0.48452285]\n",
      "7646 [ 0.40207414  0.59792584]\n",
      "7656 [ 0.42161567  0.57838433]\n",
      "7664 [ 0.47583248  0.52416749]\n",
      "7665 [ 0.59409364  0.40590636]\n",
      "7685 [ 0.46255484  0.53744515]\n",
      "7691 [ 0.53413892  0.46586106]\n",
      "7695 [ 0.4857944  0.5142056]\n",
      "7696 [ 0.56984132  0.43015866]\n",
      "7707 [ 0.41184508  0.58815491]\n",
      "7712 [ 0.57322142  0.42677857]\n",
      "7718 [ 0.54691248  0.45308752]\n",
      "7723 [ 0.54064616  0.45935382]\n",
      "7728 [ 0.40414092  0.59585908]\n",
      "7734 [ 0.49717039  0.50282959]\n",
      "7740 [ 0.51937961  0.48062038]\n",
      "7748 [ 0.44655202  0.55344798]\n",
      "7799 [ 0.40422174  0.59577827]\n",
      "7801 [ 0.54379923  0.45620079]\n",
      "7802 [ 0.49928473  0.50071527]\n",
      "7814 [ 0.49672417  0.50327583]\n",
      "7827 [ 0.43595116  0.56404884]\n",
      "7857 [ 0.4503758   0.54962419]\n",
      "7863 [ 0.50209606  0.49790394]\n",
      "7871 [ 0.50872668  0.49127332]\n",
      "7872 [ 0.48916983  0.51083017]\n",
      "7886 [ 0.43892796  0.56107202]\n",
      "7887 [ 0.5598253   0.44017467]\n",
      "7897 [ 0.46011785  0.53988214]\n",
      "7918 [ 0.46757794  0.53242204]\n",
      "7928 [ 0.4551425   0.54485749]\n",
      "7933 [ 0.45614813  0.54385187]\n",
      "7939 [ 0.40064436  0.59935563]\n",
      "7950 [ 0.50755044  0.49244954]\n",
      "7951 [ 0.55412319  0.4458768 ]\n",
      "7955 [ 0.47252128  0.52747872]\n",
      "7958 [ 0.43115814  0.56884187]\n",
      "7965 [ 0.43833451  0.5616655 ]\n",
      "7966 [ 0.42018756  0.57981243]\n",
      "7977 [ 0.51011624  0.48988375]\n",
      "8008 [ 0.4273315   0.57266851]\n",
      "8026 [ 0.42289953  0.57710048]\n",
      "8047 [ 0.53193805  0.46806195]\n",
      "8055 [ 0.50682616  0.49317384]\n",
      "8059 [ 0.47060024  0.52939974]\n",
      "8063 [ 0.48299134  0.51700866]\n",
      "8067 [ 0.42822013  0.57177986]\n",
      "8072 [ 0.44127243  0.55872756]\n",
      "8073 [ 0.55286601  0.44713398]\n",
      "8081 [ 0.58421374  0.41578626]\n",
      "8083 [ 0.58664811  0.4133519 ]\n",
      "8108 [ 0.51698284  0.48301715]\n",
      "8119 [ 0.515552  0.484448]\n",
      "8122 [ 0.41420238  0.58579762]\n",
      "8136 [ 0.57205632  0.42794369]\n",
      "8158 [ 0.55343311  0.44656688]\n",
      "8162 [ 0.4283089   0.57169109]\n",
      "8163 [ 0.50466172  0.49533825]\n",
      "8173 [ 0.57746382  0.42253617]\n",
      "8184 [ 0.47613305  0.52386695]\n",
      "8186 [ 0.59870616  0.40129385]\n",
      "8191 [ 0.42945795  0.57054204]\n",
      "8193 [ 0.51848973  0.48151026]\n",
      "8197 [ 0.48937693  0.51062305]\n",
      "8199 [ 0.44418735  0.55581264]\n",
      "8224 [ 0.42013819  0.5798618 ]\n",
      "8229 [ 0.55132518  0.44867482]\n",
      "8241 [ 0.46165697  0.53834303]\n",
      "8246 [ 0.54240939  0.45759061]\n",
      "8258 [ 0.56201699  0.43798303]\n",
      "8260 [ 0.42310988  0.57689011]\n",
      "8272 [ 0.59443466  0.40556532]\n",
      "8287 [ 0.59632933  0.40367068]\n",
      "8302 [ 0.50950805  0.49049192]\n",
      "8311 [ 0.58265775  0.41734225]\n",
      "8321 [ 0.55016489  0.44983513]\n",
      "8322 [ 0.5577313   0.44226871]\n",
      "8323 [ 0.52656205  0.47343795]\n",
      "8329 [ 0.57096909  0.42903088]\n",
      "8349 [ 0.4884991   0.51150089]\n",
      "8357 [ 0.46949866  0.53050134]\n",
      "8361 [ 0.52804032  0.47195967]\n",
      "8374 [ 0.56185293  0.43814706]\n",
      "8379 [ 0.49402859  0.50597141]\n",
      "8400 [ 0.51717167  0.48282832]\n",
      "8406 [ 0.4394459   0.56055409]\n",
      "8407 [ 0.43243666  0.56756333]\n",
      "8446 [ 0.54069602  0.45930398]\n",
      "8494 [ 0.47785249  0.5221475 ]\n",
      "8495 [ 0.49266281  0.50733716]\n",
      "8508 [ 0.5522538   0.44774619]\n",
      "8517 [ 0.48325407  0.51674592]\n",
      "8521 [ 0.5170868   0.48291322]\n",
      "8532 [ 0.48698364  0.51301635]\n",
      "8534 [ 0.59239523  0.40760476]\n",
      "8542 [ 0.5048637   0.49513627]\n",
      "8550 [ 0.51822883  0.48177116]\n",
      "8557 [ 0.53366122  0.46633876]\n",
      "8571 [ 0.51559041  0.48440958]\n",
      "8602 [ 0.51510784  0.48489215]\n",
      "8620 [ 0.44136411  0.55863589]\n",
      "8627 [ 0.41583039  0.58416961]\n",
      "8658 [ 0.40879655  0.59120342]\n",
      "8681 [ 0.41818828  0.58181171]\n",
      "8691 [ 0.56598206  0.43401793]\n",
      "8696 [ 0.57247057  0.42752941]\n",
      "8703 [ 0.40687026  0.59312974]\n",
      "8709 [ 0.56529663  0.43470336]\n",
      "8720 [ 0.58590545  0.41409455]\n",
      "8743 [ 0.4548436   0.54515638]\n",
      "8746 [ 0.40190776  0.59809223]\n",
      "8759 [ 0.43710824  0.56289173]\n",
      "8780 [ 0.50666182  0.49333815]\n",
      "8784 [ 0.51545186  0.48454814]\n",
      "8788 [ 0.52019923  0.47980075]\n",
      "8792 [ 0.43499933  0.56500066]\n",
      "8824 [ 0.50581099  0.49418899]\n",
      "8838 [ 0.43215565  0.56784432]\n",
      "8843 [ 0.46914255  0.53085744]\n",
      "8845 [ 0.42250118  0.57749883]\n",
      "8846 [ 0.59110309  0.4088969 ]\n",
      "8863 [ 0.4250886  0.5749114]\n",
      "8871 [ 0.55603908  0.44396091]\n",
      "8880 [ 0.42988853  0.57011147]\n",
      "8886 [ 0.58723235  0.41276764]\n",
      "8900 [ 0.56174156  0.43825844]\n",
      "8912 [ 0.59769506  0.40230492]\n",
      "8944 [ 0.50459971  0.49540029]\n",
      "8947 [ 0.4278843   0.57211568]\n",
      "8949 [ 0.58316975  0.41683025]\n",
      "8955 [ 0.57333092  0.42666909]\n",
      "8961 [ 0.48883749  0.51116249]\n",
      "8972 [ 0.43987195  0.56012805]\n",
      "8989 [ 0.4552474   0.54475262]\n",
      "8990 [ 0.44487574  0.55512427]\n",
      "9005 [ 0.53286579  0.46713423]\n",
      "9019 [ 0.4814833   0.51851671]\n",
      "9030 [ 0.5401944   0.45980558]\n",
      "9043 [ 0.46266625  0.53733375]\n",
      "9065 [ 0.51665641  0.48334359]\n",
      "9067 [ 0.5319648   0.46803517]\n",
      "9075 [ 0.44701657  0.55298342]\n",
      "9082 [ 0.47981152  0.52018848]\n",
      "9085 [ 0.54733851  0.45266148]\n",
      "9100 [ 0.44372147  0.55627852]\n",
      "9111 [ 0.4251053   0.57489468]\n",
      "9137 [ 0.4479137  0.5520863]\n",
      "9138 [ 0.58958891  0.41041107]\n",
      "9143 [ 0.43040543  0.56959456]\n",
      "9154 [ 0.53777447  0.46222552]\n",
      "9161 [ 0.42855111  0.57144889]\n",
      "9164 [ 0.40230485  0.59769514]\n",
      "9172 [ 0.46111261  0.53888737]\n",
      "9186 [ 0.48577119  0.5142288 ]\n",
      "9199 [ 0.58336912  0.41663088]\n",
      "9217 [ 0.42125687  0.57874312]\n",
      "9219 [ 0.55308084  0.44691916]\n",
      "9221 [ 0.45098571  0.54901427]\n",
      "9234 [ 0.52445979  0.4755402 ]\n",
      "9274 [ 0.42207678  0.57792322]\n",
      "9298 [ 0.53403187  0.46596811]\n",
      "9299 [ 0.43369662  0.56630337]\n",
      "9300 [ 0.55211016  0.44788985]\n",
      "9315 [ 0.41309681  0.58690318]\n",
      "9321 [ 0.55749622  0.44250378]\n",
      "9343 [ 0.48690757  0.51309241]\n",
      "9355 [ 0.51178501  0.48821496]\n",
      "9359 [ 0.47178428  0.52821574]\n",
      "9367 [ 0.49559359  0.50440639]\n",
      "9370 [ 0.54361881  0.45638117]\n",
      "9393 [ 0.52608741  0.47391257]\n",
      "9403 [ 0.42564367  0.5743563 ]\n",
      "9406 [ 0.48480479  0.51519519]\n",
      "9410 [ 0.41118857  0.58881141]\n",
      "9429 [ 0.49100783  0.50899215]\n",
      "9445 [ 0.59367073  0.40632927]\n",
      "9448 [ 0.5297612   0.47023878]\n",
      "9457 [ 0.55667781  0.44332218]\n",
      "9460 [ 0.5796196   0.42038036]\n",
      "9463 [ 0.47348305  0.52651695]\n",
      "9467 [ 0.44760061  0.55239938]\n",
      "9502 [ 0.44063547  0.55936452]\n",
      "9504 [ 0.42391089  0.57608909]\n",
      "9509 [ 0.43679446  0.56320553]\n",
      "9514 [ 0.55800681  0.44199319]\n",
      "9517 [ 0.50864428  0.49135573]\n",
      "9529 [ 0.52763358  0.47236641]\n",
      "9534 [ 0.43812681  0.56187317]\n",
      "9554 [ 0.46176727  0.53823273]\n",
      "9563 [ 0.4010151   0.59898488]\n",
      "9565 [ 0.42703365  0.57296635]\n",
      "9576 [ 0.48580818  0.51419181]\n",
      "9577 [ 0.59644015  0.40355985]\n",
      "9589 [ 0.53672321  0.46327677]\n",
      "9602 [ 0.52420878  0.47579121]\n",
      "9621 [ 0.47236218  0.52763782]\n",
      "9634 [ 0.5671994   0.43280061]\n",
      "9639 [ 0.49725223  0.50274775]\n",
      "9645 [ 0.53869446  0.46130556]\n",
      "9647 [ 0.4081759   0.59182408]\n",
      "9660 [ 0.50410568  0.49589431]\n",
      "9665 [ 0.41955655  0.58044342]\n",
      "9666 [ 0.48159275  0.51840723]\n",
      "9676 [ 0.57020684  0.42979314]\n",
      "9679 [ 0.40383626  0.59616372]\n",
      "9682 [ 0.54100519  0.45899481]\n",
      "9693 [ 0.48397918  0.51602083]\n",
      "9715 [ 0.54451916  0.45548083]\n",
      "9718 [ 0.519123    0.48087699]\n",
      "9725 [ 0.41838052  0.58161946]\n",
      "9743 [ 0.55091878  0.44908122]\n",
      "9746 [ 0.59008743  0.40991256]\n",
      "9751 [ 0.55155093  0.44844907]\n",
      "9762 [ 0.46786268  0.53213732]\n",
      "9774 [ 0.53932208  0.46067791]\n",
      "9781 [ 0.51977345  0.48022655]\n",
      "9790 [ 0.43623325  0.56376674]\n",
      "9813 [ 0.52357754  0.47642243]\n",
      "9844 [ 0.46365726  0.53634273]\n",
      "9856 [ 0.44012364  0.55987635]\n",
      "9868 [ 0.52977786  0.47022211]\n",
      "9870 [ 0.57874678  0.42125321]\n",
      "9877 [ 0.50918626  0.49081374]\n",
      "9880 [ 0.43200202  0.56799797]\n",
      "9881 [ 0.52350743  0.47649256]\n",
      "9893 [ 0.51291168  0.48708832]\n",
      "9915 [ 0.42721211  0.5727879 ]\n",
      "9922 [ 0.58489607  0.41510392]\n",
      "9923 [ 0.54184303  0.45815695]\n",
      "9936 [ 0.41719676  0.58280322]\n",
      "9946 [ 0.51291369  0.48708629]\n",
      "9952 [ 0.59192984  0.40807015]\n",
      "9954 [ 0.4610793   0.53892071]\n",
      "9961 [ 0.4955852  0.5044148]\n",
      "9973 [ 0.52229401  0.47770599]\n",
      "9982 [ 0.47212164  0.52787835]\n",
      "9985 [ 0.41820722  0.58179276]\n"
     ]
    }
   ],
   "source": [
    "#last minute attempt at trying to understand the predictions for which the neural net was struggling\n",
    "#I'd keep working on this approach if I had more time.\n",
    "\n",
    "for i, p in enumerate(preds):\n",
    "    if abs(p[0]-0.5)<0.1:\n",
    "        print i, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
